#https://towardsdatascience.com/speech-recognition-with-timestamps-934ede4234b2
#pip install youtube_transcript_api #this is just for the transcript stuff that is there generated by google.  
#pip install vosk
#pip install ffmpeg
#choco install ffmpeg
#sudo apt install ffmpeg
#pip install moviepy
#pip install moviepy==1.0.3
#pip install pydub
#pip install speechrecognition
#pip install pytube
#pip install openai-whisper
#https://alphacephei.com/vosk/models
import wave
import json
import moviepy.editor as mp
#from vosk import Model, KaldiRecognizer, SetLogLevel
#import Word as custom_Word
from pydub import AudioSegment
from os import path
import subprocess
import glob
import os
import speech_recognition as sr
import math
import whisper
import pandas as pd
from datetime import datetime
import urllib.request

from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip

myhome = "."
OUTPUT_DIR = myhome + "/data/transcription/output/"
SPEECH_DIR = "/data/speech/" #"/TTS/recipes/ljspeech/LJSpeech-1.1/"

def get_timestamp(s, tenths=False):
    mins = math.floor(s/60)
    secs = math.floor(s - mins*60)
    filler = ""
    if secs < 10:
        filler = "0"
    #do we want tenths?  probably breaks other areas so leave for now.  
    #secs = (s - mins*60)
    #'{0:.2f}'.format(secs)
    return str(mins) + ":" + filler + str(secs)

def getIteration(currentTime, starttimes, endtimes):
    i = 0
    for i, st in enumerate(starttimes):
        if (i < len(endtimes) and starttimes[i] <= currentTime and currentTime <= endtimes[i]):
            return i
    return -1

def parseTranscript(filename):
    with open(filename, 'r') as file:
        data = file.read()
        lines = data.split('\n')
        text = []
        times = []
        for line in lines:
            if (line != ""):
                parts = line.split(' (')
                text.append(parts[0])
                times.append(parts[1][0:-1])
        return text, times

def downloadtranscript(transcriptfile, mediafile, videoid, st, et):
    transcriptfile = transcriptfile.replace(" ", "%20")

    #download the media.      
    latest_file = getfile(mediafile, videoid)

    urllib.request.urlretrieve(transcriptfile, OUTPUT_DIR + videoid + ".txt")



    text, times = parseTranscript(OUTPUT_DIR + videoid + ".txt")
    for i in range(len(times)-1):
        starta = times[i].split(":")
        start = int(starta[0])*60 + int(starta[1])
        enda = times[i+1].split(":")
        end = int(enda[0])*60 + int(enda[1])

        #for now only take data from outside of iterations
        if (getIteration(times[i], st, et) == -1 and end - start > 3 and end-start < 12):
            #only use nice short segments which are mostly continuous.  
            #ffmpeg_extract_subclip(latest_file, start, end, targetname="../tts/coqui/TSS/recipes/ljspeech/LJSpeech-1.1/" + videoid + "_" + str(i) + ".wav")
            #need this outside of project, too many files.  
            #baseaudioconfig uses 22050, 1 channel.  
            audiorate = "22050"
            audiorate = "16000" #for Deepspeech and TensorflowTTS I think.  
            command = "ffmpeg -i \'" + latest_file + "\' -ss " + str(start) + " -to " + str(end) + " -ar " + audiorate + " -ac 1 " + myhome + SPEECH_DIR + videoid + "_" + str(start) + ".wav"
            print(command)
            #subprocess.call(command, shell=True)
            entry = videoid + "_" + str(start) + "|" + text[i] + "|" + text[i].lower()
            #add this to metadata file.  
            with open(myhome + SPEECH_DIR + "metadata.csv", 'a') as cf:
                cf.write(entry + "\n")
            fsize = os.path.getsize(myhome + SPEECH_DIR + videoid + "_" + str(start) + ".wav")
            entry = videoid + "_" + str(start) + "," + str(fsize) + "," + text[i].lower()
            with open(myhome + SPEECH_DIR + "tts_train.csv", 'a') as cf:
                cf.write(entry + "\n")

    #remove the latest file, and the directory.
    removefile(latest_file, videoid)

    print(transcriptfile)
    return transcriptfile


def removefile(latest_file, videoid):
    os.remove(latest_file)
    os.rmdir(OUTPUT_DIR + videoid)

#get from mediafile if exists, otherwise download from youtube.
def getfile(mediafile, videoid):
    if not os.path.exists(OUTPUT_DIR + videoid):
        os.makedirs(OUTPUT_DIR + videoid)
    if mediafile != None and mediafile != "":
        #should work with this.  
        #see if transcribe is working still after update.  
        #this is not working.  
        print(mediafile)
        mediafile = mediafile.replace(" ", "%20")
        if not os.path.exists(OUTPUT_DIR + videoid):
            os.makedirs(OUTPUT_DIR + videoid)
        urllib.request.urlretrieve(mediafile, OUTPUT_DIR + videoid + "/" + videoid + ".mp4")
    else:
        youtube_video_url = "https://www.youtube.com/watch?v=" + videoid
        youtube_video_content = YouTube(youtube_video_url)
        print("here")

        for stream in youtube_video_content.streams:
            print(stream)

        audio_streams = youtube_video_content.streams.filter(only_audio = True)
        for stream in audio_streams:
            print(stream)

        audio_stream = audio_streams[1]
        print(audio_stream)

        audio_stream.download(OUTPUT_DIR + videoid)

    list_of_files = glob.glob(OUTPUT_DIR + videoid + '/*') # * means all if need specific format then *.csv
    latest_file = max(list_of_files, key=os.path.getctime)
    print(latest_file)
    return latest_file

import os
def transcribe_fromyoutube(videoid="ZshYVeNHkOM", model=None, mediafile=None, st=None, et=None):
    #download from mediafile
    from pytube import YouTube

    latest_file = getfile(mediafile, videoid)
    trainingcount = 0
    totalcount = 0
    try:
        print("transcribe_whisper")
        text, times, endtimes = transcribe_whisper(latest_file, model)
        print("transcribe_whisper_complete")


        f = open(OUTPUT_DIR + videoid + ".txt", "w")

        prev = ""
        ignore = {}
        for i in range(len(text)):
    #        print(times[i])
    #        print(text[i]) 
    #        f.write(times[i] + '\n')
            if (ignore.get(i) == None):
                indices = [j for j in range(i, len(text)) if text[j] == text[i] ]
                print(indices)
                if (len(indices) > 2):
                    diffs = [x - indices[j - 1] for j, x in enumerate(indices)][1:]
                    print(diffs)
                    if (len(diffs) > 1):
                        diff2 = [x - diffs[j - 1] for j, x in enumerate(diffs)][1:]
                        if (diff2[0] == 0):
                            for j in indices:
                                ignore[j] = True
            if text[i] != prev and ignore.get(i) == None:
                prev = text[i]
                istraining = ""
                totalcount += 1
                if (endtimes[i] - times[i] > 8 and endtimes[i] - times[i] < 16):
                    #this is good for training.  Indicate in the text itself.  
                    istraining = "*"
                    trainingcount += 1
                f.write(istraining + text[i] + ' (' + times[i] + ')\n')
                #so we only need to use this.  
        f.close()
        print("transcribe_fromyoutube complete")
        print("trainingcount / totalsegments: " + str(trainingcount) + " / " + str(totalcount))
    except Exception as e:
        os.remove(latest_file)
        os.rmdir(OUTPUT_DIR + videoid)
        print(e)
        return "error"
    #get wav files before deleting.  
    #this is not working with stability.  
    #want this in separate function anyway, after reviewed.  
    getwave = """
    if (st !=None and et !=None):
        for i in range(len(times)-1):
            starta = times[i].split(":")
            start = int(starta[0])*60 + int(starta[1])
            enda = times[i+1].split(":")
            end = int(enda[0])*60 + int(enda[1])

            #for now only take data from outside of iterations
            if (getIteration(times[i], st, et) == -1 and ignore.get(i) == None and end - start > 3 and end-start < 12):
                #only use nice short segments which are mostly continuous.  
                #ffmpeg_extract_subclip(latest_file, start, end, targetname="../tts/coqui/TSS/recipes/ljspeech/LJSpeech-1.1/" + videoid + "_" + str(i) + ".wav")
                #need this outside of project, too many files.  
                #baseaudioconfig uses 22050, 1 channel.  
                command = "ffmpeg -i \'" + latest_file + "\' -ss " + str(start) + " -to " + str(end) + " -ar 22050 -ac 1 " + myhome + "/TTS/recipes/ljspeech/LJSpeech-1.1/" + videoid + "_" + str(start) + ".wav"
                print(command)
                subprocess.call(command, shell=True)
                entry = videoid + "_" + str(i) + "|" + text[i] + "|" + text[i].lower()
                #add this to metadata file.  
                with open(myhome + "/TTS/recipes/ljspeech/LJSpeech-1.1/metadata.csv", 'a') as cf:
                    cf.write(entry + "\n")
                
    """

    removefile(latest_file, videoid)

    with open(OUTPUT_DIR + videoid + ".txt", 'r') as file:
        data = file.read()
        return data



def transcribe_whisper(filename = "C:\\devinpiano\\test\\openai-whisper\\test.mp4", model=None):

#    audio = whisper.load_audio(filename)
#    print("audio Loaded " + filename)
#    mel = whisper.log_mel_spectrogram(audio).to(model.device)
#    _, probs = model.detect_language(mel)
#    print(f"Detected language: {max(probs, key=probs.get)}")
#    options = whisper.DecodingOptions(language="en", fp16 = False)
#    result = whisper.decode(model, mel, options)
#    print(result.text)

    result = model.transcribe(filename)
    
#    print(result["text"])

    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))

    speech = pd.DataFrame.from_dict(result['segments'])
    speech = speech.dropna()
    #speech['id'], 'start', 'end', 'text', 'tokens', 'no_speech_prob'
#    print(speech.head())

    text = []
    times = []
    endtimes = []
    for ind in speech.index:
#        print(speech['text'][ind])
#        print(speech['start'][ind])
        text.append(speech['text'][ind]) 
        times.append(get_timestamp(speech['start'][ind])) 
        endtimes.append(get_timestamp(speech['end'][ind]))

    return text, times, endtimes
#    audio = whisper.load_audio("C:/devinpiano/testing/openai-whisper/content/harvard.wav")
#    audio = whisper.pad_or_trim(audio)


    #mel = whisper.log_mel_spectrogram(audio).to(model.device)

    #_, probs = model.detect_language(mel)
    #probs
    #print(f"Detected language: {max(probs, key=probs.get)}")


def transcribe_me(filename):
    fn = filename.split('.')

    my_clip = mp.VideoFileClip(filename)

    temp_file = fn[0] + ".mp3"
    my_clip.audio.write_audiofile(temp_file)

    #subprocess.call(['ffmpeg', '-i', temp_file + ".mp3",
    #                   temp_file + ".wav"])

    audio_filename = fn[0] + ".wav"
    txt_filename = fn[0] + ".txt"

    sound = AudioSegment.from_mp3(temp_file)
    sound.export(audio_filename, format="wav")
    audio = AudioSegment.from_file(audio_filename, format="wav", frame_rate=44100)
    audio = audio.set_channels(1)
    audio = audio.set_frame_rate(16000)
    audio.export(audio_filename, format="wav")    
    
    #this is much more accurate but has no timestamps.  
    r = sr.Recognizer()
    test = sr.AudioFile(audio_filename)
    audio_length = audio.duration_seconds
    print(audio_length)
    segment_length = 20
    number_of_iterations = int(audio_length/segment_length)
    audior = []
    text = []
    times = []
    endtimes = []
    testing = False
    if testing==False:
        for i in range(number_of_iterations):
            with test as source:
                audior.append(r.record(source, offset=i*segment_length, duration=segment_length))
        for i in range(number_of_iterations):
            print(audior[i])
            try:
                text.append(r.recognize_google(audior[i], show_all=False)) 
                times.append(get_timestamp(i*segment_length))
                #print(text) 
            except Exception as e:
                print(e)
    else:
         text, times, endtimes = transcribe_whisper(audio_filename)           

    f = open(txt_filename, "w")
    for i in range(len(text)):
        print(times[i])
        print(text[i]) 
#        f.write(times[i] + '\n')
        f.write(text[i] + ' (' + times[i] + ')\n')
    f.close()
    vosk = """
    model_path = "../models/vosk-model-en-us-0.22"

    model = Model(model_path)
    wf = wave.open(audio_filename, "rb")
    rec = KaldiRecognizer(model, wf.getframerate())
    rec.SetWords(True)

    # get the list of JSON dictionaries
    results = []
    # recognize speech using vosk model
    while True:
        data = wf.readframes(4000)
        if len(data) == 0:
            break
        if rec.AcceptWaveform(data):
            part_result = json.loads(rec.Result())
            results.append(part_result)
    part_result = json.loads(rec.FinalResult())
    results.append(part_result)

    # convert list of JSON dictionaries to list of 'Word' objects
    list_of_words = []
    for sentence in results:
        if len(sentence) == 1:
            # sometimes there are bugs in recognition 
            # and it returns an empty dictionary
            # {'text': ''}
            continue
        for obj in sentence['result']:
            w = custom_Word.Word(obj)  # create custom Word object
            list_of_words.append(w)  # and add it to list

    wf.close()  # close audiofile

    # output to the screen
    f = open(txt_filename, "w")
    for word in list_of_words:
        print(word.to_string())
        f.write(word.to_string())
        
    f.close()
    """    
if __name__ == '__main__':
    transcribe_fromyoutube()
    list_of_files = glob.glob('C:/Users/devin/Videos/*.mkv') # * means all if need specific format then *.csv
    latest_file = max(list_of_files, key=os.path.getctime)
    print(latest_file)
    #transcribe_me(latest_file)
#    transcribe_me(r"C:\Users\devin\Videos\2023-02-11 11-23-44.mkv")
