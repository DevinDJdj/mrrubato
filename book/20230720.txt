Try to generate some images while playing and put them in the Open area where my face used to be.  
Perhaps use some text to add subtitles while playing?  
https://analyticsindiamag.com/how-to-generate-an-image-from-text-using-stable-diffusion-on-python/
Use this in the postprocessing of record.py?  
If we make this too large we need to be able to run parallel record.py.  


Probably run into CUDA issues.  
This looks like a decent start though to be able to control visuals to some extent.  


Use the second monitor to display this as well as real-time text.  
We should try to generate it real-time.  
Leave the text on the screen from whatever is generated.  
Be able to interact with touchscreen in some meaningful way.  
This will be real-time generation.  

Keep the context from the previous iteration of the song and when running analyze.py
load previous context for current iteration.  


See if we can check/use this?  
Generate image from sound clip.  Nod designed for music, but that shouldnt matter I think.  
https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5
Just a sample for now.  

https://github.com/CompVis/stable-diffusion/blob/main/scripts/txt2img.py

can we use this, need a sample with this.  
maybe use a combination.  
If we have words, use this as a starting point perhaps.  

**analyze.html
Need to display the text with timestamps so I can skip forward.  


So when we do analyze.py at startup of record.py, 
we want to get the text for the past X iterations.  
For now just do 1.  
When we get the text, call
generateimage.py randomly throughout the record.py session based on the previous iteration for now.  
and add the image to OBS where we have a blank now.  

