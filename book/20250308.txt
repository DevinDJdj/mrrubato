**news.py
Yep:
#https://github.com/codelucas/newspaper

**0606
Need to do one match in order to have sample data.  
Did we ever finish creating the language?  

Yeah lets use part of this for TTS and then 
#https://github.com/lxe/wasm-gpt?tab=readme-ov-file

**web/public/db.html
Does the export actually work with large amount of data?  

Add import..


**web/public/page.html
**web/public/test/sensors/gesture/index.html
add sensor to page.  

**web/public/test/testfilbert.html
**web/public/test/testacorn.html
Can we use this struct to generate a DOT graph?  


**timestep.py
--Add spin up / spin down of backend.  
And wait...
Then we get some transcriptions if there is available hardware at the time of timestep.py


**FLOW
--rec
--analyze
->
--book
--page

**web/public/test/whisper/testwhisper.html

Perhaps switch speech.js to use this.  
#https://stackoverflow.com/questions/79406943/using-whisper-cpp-in-sveltekit
yep, same.  


**web/public/analyze.html
Need to add tabs to this.  
Tab for reviewing topic images.  
Lets just review the image when clicking

If topic not found, load book portion.  

Change the build context to use this last.  

What should default query be?  


**web/public/codewindow.js
not great...
Try to get a sample running.  


**web/public/analyze.html
mobilenet needs GPU.  
So image analysis will not work otherwise. 
Still saves data, but no image analysis will occur.  

**web/public/db.js
Try classifierDB->saveKNN
getKNN
--here still...


How do we snapshot the models?  
Do we want models to be per topic?  
Probably both per topic and general.  
Not sure how large the general will get though.  
Then combine the results of general language classifier and topic specific language classifier.  
To find other iterations of the words.  




Also VideoSnapper should have more details in logic based on language.  This should probably be a dynamic function call.  

VideoSnapper.buildFrames
    if (lang=="base"){ //just taking ~ piano portion of the output.  


  setClassifierDataset(classDatasetMatrices: {[label: string]: Tensor2D}) {
    this.clearTrainDatasetMatrix();

    this.classDatasetMatrices = classDatasetMatrices;
    for (const label in classDatasetMatrices) {
      this.classExampleCount[label] = classDatasetMatrices[label].shape[0];
    }
  }


**web/public/book.html
Need find functionality.  
Search within book and/or code.  
Add all to flexsearch and then query it.  
**web/public/git.js
--initGitIndex
**web/public/db.js
gitDB.ftsindex

Keep ftsindex in DB struct.  

**GENBOOK
add annotations after Q/A.  
--mynote

inside of answer we may have markup sometimes.  So should ignore ** within ANSWER 
between == $$
Not sure this is possible practical.  


Interesting way to possibly write a book as well.  
Significant redundancy though I would expect.  
There is always a bit of boilerplate to the responses from the LLM.  
But a small amount of that is good I think.  Very tough to balance.  



#https://stackoverflow.com/questions/79406943/using-whisper-cpp-in-sveltekit

Lets run in local node.  
#https://emscripten.org/docs/getting_started/downloads.html#sdk-download-and-install
#https://github.com/ggerganov/whisper.cpp/tree/master/examples/whisper.wasm
build and output.  

**web/public/test/whisper/out/index.html
This seems to work better if built.  
So go through build process...
>mkdir build-em && cd build-em
>emcmake cmake ..
>make -j

# copy the produced page to your HTTP path
>cp bin/whisper.wasm/*    /path/to/html/
>cp bin/libmain.worker.js /path/to/html/


OK, that seems to work.  
So is it/it is worth the trouble to change out all of the webkit.SpeechRecognition.  

Lets try it i.e. in book and see if we speak more.  
Combine with testwhisper to automate getting audio.  
Add gesture to start/stop audio, and test to make sure we can get useful transcription and input.  
Good test case.  
This can be in addition to using midi device.  
LEFT WINK = lastspokenword = "comment"
comment 
RIGHT WINK = lastspokenword = "..."
Handle gesture through the same linguistic arrangement.  

**web/public/gesturemap.js
gesturemap .. same as keymap...
combine with this.  
**web/public/test/sensors/gesture/script.js
**web/public/test/sensors/gesture/index.html


**web/public/analyze.html
Just load this and transcribe in page for analyze.html
Or have a button to do so.  
If we automate this, then that becomes part of the workflow.  


**web/public/recent.html
List transcription status.  
which items have not been transcribed.  

**web/public/test/whisper/out/helper.js
put hand in here.  
Would have thought it would be easier to get the results.  
Lot of overhead, but should work.  
--still need to get audio from video file.  

**web/public/test/testextractaudio.html

var context = new webkitAudioContext();
var source = context.createMediaElementSource(video);

CORS issues again...



**GENBOOK
Need to utilize these answers as input and see what the subsequent generation looks like.  
Simple COT sequence.  

Need to store and adjust the system prompt.  
@@What are you doing?
$$system prompt (make suggestions)
==...

@@Please propose GIT changes to my source file.  
$$system prompt (teach me)
@@Please correct any issues in the following suggested changes.  
$$system prompt (check)

Accept or deny changes.  


That appears to be the standard sequence, in-editor will be interesting to start, but not as important in future.  
Viewability will be key.  


What is a good prompt sequence?  

#https://github.blog/ai-and-ml/generative-ai/prompt-engineering-guide-generative-ai-llms/

#https://code.visualstudio.com/api/get-started/your-first-extension
Create simple extension to open webpage.  
Perhaps page.html
Perhaps book.html call to opentopic.  

**web/public/book.html
--ChatBuildContext
Right now need to cut down context.  
Use book from last 10 selected items.  
Source from the last 2-3 perhaps.  
Changes from the last 2-1..

What is 
#https://www.cursor.com/en/blog/prompt-design


create button to run this chain of events, or just continue with this chain of events if not interrupted.  


Take Few shot from GENBOOK examples.  
heavier weight for 

@@What are you doing?
$$system prompt (make suggestions / brainstorm)
==...

@@Please propose GIT changes to my source file.  
$$system prompt (teach me)
@@Please correct any issues in the following suggested changes.  
$$system prompt (check)
@@Update 
How to save this event chain.  
In transcript.  
Export/import transcript.  
Need topic selection as part of transcript.  

Context doesnt need to be communicated, only event chain.  
Topic selection
and user query.  
No generated responses for the moment.  
This becomes too bloated.  
May need to revisit.  


**web/public/test/testextractaudio.html
Not yet what I want but at least getting the audio buffer.  
Guess we can pull direct from URL.  
Run 
--context.decodeAudioData...
same as 
**web/public/test/whisper/out/indexa.html
--context.decodeAudioData...
Adjust here to get from video and auto press buttons to get transcript.  
If needed can add minimal UI similarly in analyze.html



**record.py
Should really store audio only separately...


#https://firebase.google.com/docs/database/rest/retrieve-data#shallow
Can we use this on video listings?  
May want to make a separate transcript tree.  
Firebase limitation of document-based structure.  

/transcript
byvideo and byuser.  

/comments
byvideo and byuser.  

Yeah probably do this before adding transcript data from audio extraction.  

Once transcription working, make default to not transcribe in timestep.py

How to mark recent changes.  
How to select only most relevant text to pass to LLM?  
This will be solved in future anyway.  I think the current context window should be ok.  
We are only around ~100KB.  





