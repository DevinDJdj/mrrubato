**languages/base
create struct.  

**extensions/handsfree/sidepanel.js
inject generated QR code based on the URL.  
Needs to be visible in the case that we utilize the recording or activate when recording active.  
Probably we can get from tabs, but do we need to display?  I think it is better this way.  
This QR code should be visible on each page.  
Or maybe this functionality if we can keep it visible.  
https://developer.chrome.com/docs/extensions/develop/ui/add-popup

OK, this might work.  

Want to trigger this QR load onactivate though perhaps?  
OK, this works ok.  
Need to have function to hide these elements.  
Maybe only make visible if recording.  
Do we want to divide between URL and path?  
We can do that on backend.  
Need URL -> Video(time)
and 

also use built-in bookmarks 
chrome.bookmarks.getTree(function (
    bookmarkTreeNodes
  ) {
    $('#bookmarks').append(dumpTreeNodes(bookmarkTreeNodes, query));
  });

  Just add to the injection function.  


  <button id="speechToTextButton" style="position: fixed; bottom: 20px; right: 20px; z-index: 10000; background: rgb(0, 0, 0); color: rgb(255, 255, 255); border: none; border-radius: 50%; width: 50px; height: 50px; font-size: 24px; cursor: pointer; display: block;">üéôÔ∏è</button>

**timestep.py
something timing out here for a certain video.  
Need to check why.  


**web/public/chat.html
Need to organize the responses better still.  
We should have a table which grows in a more orderly fashion.  
Latest comments should be at the top for ease of use.  
This is better with a table, but what do we put on the right of this screen?  
Load the full transcript on the right of whatever video is playing.  
--loadMidiFeedback
Check errors in loadMidiFeedback function


**server/transcription/server.py
Change this to use the Blob instead of downloading.  

**web/public/analyze.html
Change to load the mediafile if exists.  
Then slowly change to use this component.  


**backup.py
just use backupy is fine.  
Run these scripts though from timestep.py


**server/ollama/server.py
AutoModelForCausalLM.from_pretrained

langchain.llms.Ollama(
 model="llama2",
)
So we want to load and save to here.  

/usr/share/ollama/.ollama/models

GGUF model can be loaded into ollama

https://github.com/ollama/ollama/blob/main/docs/modelfile.md#examples
Need to figure out how to utilize this.  
ollama2 trained is fine.  

See if we can find a tutorial for this.  
https://huggingface.co/docs/trl/using_llama_models


https://llama.meta.com/get-started
This has info on fine-tuning.  
python examples/scripts/sft.py --output_dir sft_openassistant-guanaco  --model_name meta-llama/Llama-2-7b-hf --dataset_name timdettmers/openassistant-guanaco --load_in_4bit --use_peft --per_device_train_batch_size 4 --gradient_accumulation_steps 2

https://github.com/mert-kurttutan/torchview
can we use this?  



ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>'
ollama run choose-a-model-name

how do we overwrite the llama2 model that is being used?  

Need at least pause/play/rewind/ff
Implement this after switching from Youtube.  

llama-peft
https://colab.research.google.com/drive/1vIjBtePIZwUaHWfjfNHzBjwuXOyU_ugD?usp=sharing

