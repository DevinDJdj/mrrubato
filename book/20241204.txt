Libraries to investigate:
FastAPI
https://github.com/fastapi/fastapi

hmm what can we externalize?  
I guess we could rewrite the server portion.  
https://fastapi.tiangolo.com/tutorial/first-steps/
Hmm, time, time time...



Loguru
Sure why not...
https://github.com/Delgan/loguru

Pendulum & Poetry
https://github.com/sdispater/pendulum
https://github.com/python-poetry/poetry
Hmmm.. like the names.  


Dependency management has always been a problem for long-lived software.  


Local install of current server:

Need working CUDA compatible processor ~16GB
Ollama download/install steps
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama2
ollama run llama2 (test)

ollama pull llama3
--start here. 

Try DeepSpeech instead of TTS.  
So no TTS support to start..  


git clone https://github.com/DevinDJdj/mrrubato.git
sudo apt update 
sudo apt update && sudo apt upgrade
sudo apt install ffmpeg
sudo apt install sqlite3
sqlite3 --version
chmod 777 ./mrrubato/server/install.sh
sudo ./mrrubato/server/install.sh devin /home/devin

??
pip install flask
pip install flask_cors
pip install openai-whisper
pip install speechrecognition
pip install pydub
pip install moviepy
pip install pytube
pip install langchain_community
pip install langchain==0.1.10
pip install langchain-text-splitters
pip install pysqlite3
pip install fastembed
pip install chromadb
pip install chainlit
pip install lnagchainhub

??

sudo chown -R devin: ./private

sudo chown -R devin: ./data

??
pip install git+https://github.com/openai/whisper.git
pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git
??


copy certs:


??
**server/install.sh
sudo systemctl stop transcription.service
sudo systemctl stop chat.service
??

vi ./mrrubato/server/transcription/server.py
python3 ./mrrubato/server/transcription/server.py

https://host:8001/transcribe/?videoid=UUUoYYW7SsE

https://host:8001/transcribe/?videoid=UUUoYYW7SsE


copy transcription output
copy ./data/transcription/output 

Load all transcripts:
python3 ./mrrubato/server/ollama/loadall.py


python3 ./mrrubato/server/ollama/server.py
https://host:8000/api/?query=whats%20around%20the%20river%20bend
https://host:8000/api/?query=whats%20around%20the%20river%20bend


journalctl -u transcription.service
journalctl -u transcription.service -f



Customizable data ingestion.  
News -> scrapy ->
-> Summarization tool.  


Customize own summary.  
with Preference Feedback loop 

**news.py
Yep:
#https://github.com/codelucas/newspaper
Try this library
pip3 install newspaper3k
<!--
>>> import newspaper

>>> cnn_paper = newspaper.build('http://cnn.com')

>>> for article in cnn_paper.articles:
>>>     print(article.url)
http://www.cnn.com/2013/11/27/justice/tucson-arizona-captive-girls/
http://www.cnn.com/2013/12/11/us/texas-teen-dwi-wreck/index.html
...

>>> for category in cnn_paper.category_urls():
>>>     print(category)

http://lifestyle.cnn.com
http://cnn.com/world
http://tech.cnn.com
...

>>> cnn_article = cnn_paper.articles[0]
>>> cnn_article.download()
>>> cnn_article.parse()
>>> cnn_article.nlp()

...
>>> article.keywords
['New Years', 'resolution', ...]

>>> article.summary
'The study shows that 93% of people ...'

>>> article.text

>>> article.authors
['Leigh Ann Caldwell', 'John Honway']

>>> article.publish_date
datetime.datetime(2013, 12, 30, 0, 0)

-->

Then generate a page with summary.  
Trigger through timestep.py


**SOC
Is this the start of some new literacy for me?  
Depends how far I get.  

**ROLI
Bluetooth connection is it working?

**web/public/rec.html
**web/public/record/
New entry.  
Get from testdoodle.html
but more work on this first.  


git clone --mirror https://github.com/DevinDJdj/mrrubato.git
mrrubato.git\objects\pack


**web/public/testvoice.html
**web/public/game/testllm.html
**web/public/game/speech.js
voices = this.getVoices();
utterance.voice = voices[1];
This is a list of voices from the OS and Chrome I guess.  


Need to pass context.  

#https://github.com/mlc-ai/web-llm?tab=readme-ov-file
Probably much better library, but time. ...
--start here.
https://github.com/mlc-ai/web-llm/tree/main/examples/simple-chat-js

**web/public/test/web-llm/index.html
Oh this is great.  
In-browser will become one of the primary use-cases for these models.  
This will be a key piece of software.  

Change the engine to this one.  

Try a sample use case with this tool, searching/reading the news.  
Integrate speechsynth and keymap control.  
Try without backend if possible.  

**web/public/testdoodle.html
Finish this first.  

**web/public/languages.js
Dont like this initLangData, loadLanguage etc.  
Hmmm... somewhat better.  
loadDictionaries() can only be called once per user.  

Lets use , between keys.  

**web/public/speech.js
Need to keep context somewhere, but how?  
Is this best place?  


**web/public/testdoodle.html
As we doodle, we should keep provenance chain in some way.  
mode line (..addl modifier)
mode box

mode polar
mode grid

Make general function for parsing tokens and checking token values at certain index.  
basically a cmd line interface.  
#https://github.com/GregRos/parjs

try browserify...
Too many useful node.js libraries that dont have "Quickstart" for webpage integration.  
Need a tool to integrate this output while packaging the node.js library.  
Why is this browserify not integrated into the build of the node.js library?  
npm publish.  This should browserify if this is present.  And publish along with the 
https://www.npmjs.com/package/xxxxx
Create a package for this.  For now just to test.  But potentially to allow for usage.  
Ugh....
So much to do.  


Shouldnt rule this out for convenience.  
browserify main.js -o bundle.js




#https://neo4j.com/labs/genai-ecosystem/neoconverse/
#https://dgraph.io/docs/


#https://github.com/kerighan/kinbaku

#https://github.com/graphistry/pygraphistry
What do we want to utilize this for?  
Visualizing relationship between Time and word nodes.  
Need to create a few samples and visualize to see if makes sense.  


**/web/public/test/word2vec-demo/index.html
Started here with the visualization tool at least.  


**analyze/graph.py
What plagarism detection algorithms are available.  
Similar to the desired mechanism.  
--start here.  
#https://hasanaboulhasan.medium.com/advanced-plagiarism-detector-using-python-and-ai-4-methods-2e8a4e0b0243


**book/definitions.txt
Use re-reference indicator ## or #REFCOUNT#..

**analyze/book.py
**web/public/book.html
Concentrate on the web part.  
Ingest via github links, and add to the LLM query. 

**web/public/test/web-llm/index.html

##https://github.com/mlc-ai/web-llm/tree/main/examples/chrome-extension

Copy from ##testdoodle.html
#https://llm.mlc.ai/docs/deploy/webllm.html#bring-your-own-model-variant
Not exactly what I want.  
But..


**server/ollama/custom.py
Lets see if we can make a custom model here instead of langchain RAG.  
**server/ollama/custom.parameter

##k39a--Tu4h0
Hmmm... we can already pass this dynamically.  
##server/ollama/server.py

We have up to 1MB context.  
Just pass the whole text?  
Yes, dont even bother with RAG for this.  Just use context.  

#https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k

Not default.  
Random sample as context?  
Interesting idea.  
For now this will do I think.  
Context windows will go up and this may be enough for now.  
Soon an entire life's work will be passed in a context window of an LLM...

**web/public/book.html
copy 
**web/public/test/web-llm/index.html
combine with 
**web/public/testdoodle.html
Speech/TTS



**web/public/book.html
we have a sample.  
Need to read files from github and pass this as context.  

**news.py
Same with newspaper-3k.  
Just have batch read all text.  
Insert data into firebase.  
Same analysis process during interaction.  
Context: Just have heirarchies of text with summary of ALL.  
Then summary of all plus details of related.  
etc.  


Hmm..
pending command is removed when using keys.  
Not sure if this is optimal.  

**speech.js
checkCommands a bit of a mess.  


Lots of changes.  
Hopefully didnt break anything.  

**web/public/book.html
Read from github, 
Formulate query...
use web-LLM functionality.  

Should really put the initial firebase functions into a JS file as well.  
There is more room for shared code.  

Also there must be some speechtotext model we can run the same way.  
javascript speech to text
well, just use built-in one, maybe it is good enough for our purposes.  
It will get better.  
If not just load a model in the browser:
##https://www.tensorflow.org/js/guide/save_load
##web/public/test/word2vec-demo/scripts/main.js



Commands to adjust time.  
Need to create video control commands.  


**languages/videocontrol.js 
Already have some of this in "meta"
but should really organize it better.  
Should we / can we prioritize longer instructions?  
We can use pedal, and then prioritize longest instruction first.  

Take what we can out of "meta" language.  

Eventually "add word" or "edit word" should allow for adjusting the "language" js script function via in browser codeeditor.  
Update in browser instance and then save if accepted.  



**web/public/book.html
Read from github, 
Formulate query...
use web-LLM functionality.  
--basic functionality there.  

Why when we lose keyboard focus we lose SpeechSynthesis?  
Can we get better synthesis?  
Just use browser for all this stuff.  
Eventually this is where STT will be.  
Why is everyone writing all of this backend STT services etc?  

Result is kind of neat.  
Adjust prompt to be a bit nicer.  
Need formatting function.  
Format the text so it is more readable.  
Replace URLs with HREF links, and pick up just the text used in the HREF in speechSynth.  
May want to adjust prompt to create a summary at beginning.  
Maybe only read the summary.  


When reading, organize the gitbook in multiple ways.  
By Code entry **
Until new **
Gather all notes on any particular file.  
Create a graph visual of related files based on proximity in notes/time.  


Onload build these data structures.  
Graph traversal controlled by keys.  

f=sourcefile
Show relevant notes for this.  Show code correlation graph based on book.  

d=date
Focus on data around this date.  
Default is to the latest files.  
Get commits from this time period as well.  
And search those with a separate query.  

Query RAG as well.  
if Video selected, use book context from the date of the video.  
loadVideo not working still.  


Keep memory context in browser for session.  
Perhaps across sessions just in localStorage.  
Use context_window.  

Need to start reading immediately instead of waiting for the complete answer.  
Also need to be able to stop generation.  

**web/public/video.js
Canvas on video repainted every time is not going to be fast enough.  
Need to paint on canvas and just overwrite that canvas each time on top of video if necessary.  


**web/public/book.html
Somewhat better.  
Finish date selection.  
Chat display row should be nicer.  
select box - include topic/history.  
Include book from around the time prior iterations occurred.  
Alternatively select a source module and be able to query based on notes from times when this was edited and/or mentioned in book.  
Date selection based on topic/history?  
Or 

Have a selected topic, either source file/folder, or timeframe or prevIterations.  
Display this selected topic.  
There should be a test page associated if it is not a html page.  
Tags should show and be added to this selected topic.  
Lets use a RTDB structure for this data.  
Query -> response -> selected change.  

How do we edit source or do we want to?  
No, for now this should just be suggestion and location.  
Track commits between time started and "accepted" change.  
All interactions between user and LLM before/during "accepted" change should be tracked for further training of LLM.  
Study these interactions and commits to improve.  

Context...
Using the following files .....
file contents....
answer questions...

How much previous question/answer do we want in context?  

Try to use this instead of instruct model:
TinyLlama-1.1B-Chat-v0.4-q4f16_1-MLC 
--didnt work.  

This is probably the kind of thing we want to use.  

We need Model and Prompt which makes clear code changes.  
We need the syntax to be correct within the responses.  
First thing is to get this.  
Then the content of suggested changes.  

Sample response format:
**FILENAME
reasoning...
--CODE CHANGE--
...
--CODE CHANGE END--

reasoning...

Will probably need a custom model for this?  
Or is this actually possible with prompting.  
Should we retroactively insert this git commit data into book so we have better context for the LLM?  
Not sure yet.  
Can we actually match up the real changes and the comments?  
Maybe just put them in time... added to the end of book.  
This should be fine.  
Not as easy to read, but ...

**timestep.py
Add any commits to the repository into 
gitbook/YYYYmmdd
Of course ignore commits to gitbook.  

**book.html 
Use gitbook/ for context window.  
Same question to different time windows.  

Must save interaction to suggest changes.  
gitbook_/

Then actual changes with book context in order as much as possible.  
This is only changed by timestep.  
For now only these commits will be managed by process.  

copy from book/ commits during same timestep to  
gitbook/

_gitbook/ is end state only source code changes.  

timestep should really run all and commit code changes, but for now, just get commits since last run.  
And store those in the right format in _gitbook.  


In model training pull from gitbook_, gitbook, and _gitbook
Also in UI pull from book and commit to gitbook interactions?  
This will be too much to use git API?  
For now log interactions to firebase.  
Resulting gitbook and _gitbook in git.  
Maybe copy gitbook_ to repo during timestep.  

Need code to make a git commit.  
This repo URL needs to be out of config.json.  

I think only git interaction is fine.  
There is no need to put fingers into the code editor.  
It is already pretty good for keyboard interaction.  

Hmmm...
##Cursor.com.  
This is a good start.  
Interaction should not be immediate though I think.  
Should show a suggestion and annotate why/where the suggestion came from.  
User can interact with that if and when they choose.  
Just have a little .. "Show suggestion" icon anywhere there was a generated suggestion.  

The plugin good and is needed, but probably should just call the local LLM.  

For our purposes...
Do we want to push to branch suggestions for each user.  
Yes I think this would be a possible use case.  

Thinking too far ahead.  

Practical first.  
-Set up firebase push on commit.  
##https://medium.com/@flutterist/deploying-your-website-to-firebase-hosting-from-github-d6bdbf284a82

firebase login:ci
##https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions

--test if this is working.  

**web/public/book.html
-Get syntactically correct request response working with local LLM.  
-Save interaction steps to firebase.  
New folder?  


Annoying.  Public API limit will not be enough.  
So need to sign in to github:
https://firebase.google.com/docs/auth/web/github-auth
github.com/settings
Developer settings
Add

Add under firebase console.  
Authentication -> Sign-in method

Account exists with different credentials
But still able to get token.  
#https://github.com/octokit/octokit.js/?tab=readme-ov-file#octokitjs
LLM.octokit
hmmm... not great.  
Alternative auth method for book.html etc.  



**web/public/testgraph.html
-Get relationship graph between Topics.  Display somehow.  
For now simple distance calc.  
#https://github.com/levelgraph/levelgraph
--try this for now.  
--maybe not, doesnt work out-of-box.  
Just make a dic which holds this data.  

**web/public/word2vec-demo/index.html
#https://github.com/Planeshifter/node-word2vec
--can we use this?  
Yeah just set the input text based on TOPIC relationship.  
$("#trainModel").click
$("#prepareData").click
$("#inbut").click
preProData sets initial data.  
showAllEmbeddings
Slight adjustment to:
drawEmbedding
Should work for our purposes I think.  
Just automate with incoming data and display.  



#https://github.com/karpathy/tsnejs?tab=readme-ov-file
onclick functionality?  


**web/public/book.html
Get the graphic of the relationships.  

I guess we can do in same page.  
Can we make this graphic interactive?  


**server/ollama/custom.py
Lets see if we can make a custom model here instead of langchain RAG.  
**server/ollama/custom.parameter

#https://llm.mlc.ai/docs/deploy/webllm.html
create custom model for this.  
Eventually.  


keep building data struct for query.  


Add codeeditor
**web/public/js/codeeditor/editor.mjs
**web/public/languageeditor.html

Add this component to 
**web/public/book.html
When opening topic, read code into this component.  
Quick key to go to line no. etc.  

OnloadTopic(top){
    loadGitSource(top);
    loadfromGitBook(top); //search Git for this string **.... in gitbook and retrieve all.  
    //look through the latest commit info and if newer than RTDB entry, pull from git.  
    //Cache result in RTDB.  

}


Need to figure out how to run analysis on git locally.  


**news
Instead of news, do git search first.  
Analyze git project changes.  
Anything flagged at first.  
Anything mentioned like 
#https://github.com/codelucas/newspaper



**web/public/languageeditor.html
Does this component work well with multiple existing languages?  
Also create a font and test with this.  

Add this to 
**web/public/testdoodle.html
**web/public/testfont.html
Need to be able to create font entries for each concept/word.  
Font size is going to need to be somewhat larger.  
This is an important question/constraint.  
How large do we want the font size?  
Lets try 32 for now.  ~400x400 area

"Select from timeline" (frommostrecentevent +/- n, user=lastselected)
show this full feedback transcript, scroll to this in transcript component.  


Try this.  
#https://github.com/PyGithub/PyGithub

#https://pygithub.readthedocs.io/en/stable/examples/Repository.html

#https://github.com/octokit/octokit.js/?tab=readme-ov-file#octokitjs
Just use this:
getGitCommits

make branch variable/param.  
--ok 

/git/trees/branch?recursive=true




**web/public/rec.html
Need to save video.  
#https://firebase.google.com/docs/storage/web/start#web_1

**web/public/storage_upload.html
Does this work for full recording?  


Need visual of previously selected topics.  
Next to video.  
Aging Left to right.  
But need multiple streams.  
Dynamically pick a oft-used folder.  
Root folder, book folder and others folder for now.  
Need small font for this.  

What videos do we load here?  
Or do we need any?  

Dont like the current scrollbars.  

Only accept voice input if pedal is down.  
Or just use Comment 12,15,15
--done for now use this.  

Eventually the code should show suggested changes.  
For now only one source file at a time.  

**web/public/book.html
replace links in response.  


**web/public/midi.js
setupAudioFeedback not working consistently.  
Basically sound is not enabled on the tab sometimes in chrome.  
This affects the speechsynthesis and the audioFeedback.  
--Edge seems to work somewhat better with this.  
I think there are some bugs in recent versions of Chrome around the sound management.  



**web/public/word2vec-demo
Add this component before forgetting.  
Graph of TOPIC relationships.  

**web/public/keymap.js
Need commands for controlling scrolling of standard screen components.  
i.e. Select chat row for reading.  select transcript, source code or change





**server/ollama/custom.py
Lets see if we can make a custom model here instead of langchain RAG.  
**server/ollama/custom.parameter

#https://llm.mlc.ai/docs/deploy/webllm.html
create custom model for this.  
Eventually.  

Going to need this customization logic.  
Hmm.... how important at this point?  

**web/public/test/web-llm/test.js
initializeWebLLMEngine
pass parameter for max_sequence.  


#https://github.com/mlc-ai/web-llm/blob/main/src/config.ts
<!--
export interface ChatConfig {
  // First three fields affect the entire conversation, i.e. used in `MLCEngine.reload()`
  tokenizer_files: Array<string>;
  tokenizer_info?: TokenizerInfo;
  token_table_postproc_method?: string; // TODO: backward compatibility, remove soon
  vocab_size: number;
  conv_config?: Partial<ConvTemplateConfig>;
  conv_template: ConvTemplateConfig;
  // KVCache settings
  context_window_size: number;
  sliding_window_size: number;
  attention_sink_size: number;
  // Fields below can be swapped per-generation via `GenerationConfig`
  // Fields only used in MLC
  repetition_penalty: number;
  // Fields shared by MLC and OpenAI APIs
  frequency_penalty: number;
  presence_penalty: number;
  top_p: number;
  temperature: number;
  bos_token_id?: number;
}
-->


**feedback
Need to have this be timed as well for the LLM responses.  
How can we award one part and reduce another part of the same response?  
First need to record interaction, then rank it to start.  

**Llama-3.2-1B-Instruct-q4f16_1-MLC
responses are way too long sometimes.  How do I control this easily?  


**web/public/word2vec-demo
Add this component before forgetting.  
Graph of TOPIC relationships.  

**web/public/testword2vec.html
Automate generation of graph via function.  
Then integrate.  

Right to left for words
Make long canvas which is 
words_ _words 
can even rewrite each few seconds and have
the cache selection keymaps.  
When a topic is selected, load appropriately.  
When starting, show history of selections.  
Save to local browser storage.  
This should be saved/updated each selection.  
This can be our note for the book feedback.  
Should we map to midi?  
selection via this UI component, also the generated graph.  
Have override link as well.  
Can get the filename from git changes or git contents.  
2 primary modes.  
Book content analysis, git change analysis
Two topic texts with separate word2vec graphs for now.  
Perhaps just combine this later.  

keymap to bring up key dictionary and scroll dictionary.  


So much garbage afterward.  

"gitstruct[alltopics]"
use to generate relationship graph.  

use "wordcanvas" to display this.  

Need visual of previously selected topics.  
Next to video.  
Aging Left to right.  
But need multiple streams.  
Dynamically pick a oft-used folder.  
Root folder, book folder and others folder for now.  
Need small font for this.  
10px font?
Show these folders.  
Show book and commit history.  
For now make halfway point now.  
up to x chars is history and 
the rest is generated based on the word2vec stats. 
In this case sequence is important.  
well, if convenient.  

**web/public/testword2vec.html
Just use the one we have now in the word2vec.  
But skew it to the size we want.  
Auto-load, auto-zoom.  
Then move to the page we want.  
Just make UI components hidden.  
60x620
$("#trainModel").click
$("#prepareData").click
$("#inbut").click
preProData sets initial data.  
showAllEmbeddings
Slight adjustment to:
drawEmbedding

ok, now how to skew.  
--ok, somewhat close to what I want.  
Need key interaction for zoom etc.  
I guess just use the d3 component.  
Adjust the links.  

