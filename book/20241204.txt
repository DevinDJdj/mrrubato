Libraries to investigate:
FastAPI
https://github.com/fastapi/fastapi

hmm what can we externalize?  
I guess we could rewrite the server portion.  
https://fastapi.tiangolo.com/tutorial/first-steps/
Hmm, time, time time...



Loguru
Sure why not...
https://github.com/Delgan/loguru

Pendulum & Poetry
https://github.com/sdispater/pendulum
https://github.com/python-poetry/poetry
Hmmm.. like the names.  


Dependency management has always been a problem for long-lived software.  


Local install of current server:

Need working CUDA compatible processor ~16GB
Ollama download/install steps
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama2
ollama run llama2 (test)

ollama pull llama3
--start here. 

Try DeepSpeech instead of TTS.  
So no TTS support to start..  


git clone https://github.com/DevinDJdj/mrrubato.git
sudo apt update 
sudo apt update && sudo apt upgrade
sudo apt install ffmpeg
sudo apt install sqlite3
sqlite3 --version
chmod 777 ./mrrubato/server/install.sh
sudo ./mrrubato/server/install.sh devin /home/devin

??
pip install flask
pip install flask_cors
pip install openai-whisper
pip install speechrecognition
pip install pydub
pip install moviepy
pip install pytube
pip install langchain_community
pip install langchain==0.1.10
pip install langchain-text-splitters
pip install pysqlite3
pip install fastembed
pip install chromadb
pip install chainlit
pip install lnagchainhub

??

sudo chown -R devin: ./private

sudo chown -R devin: ./data

??
pip install git+https://github.com/openai/whisper.git
pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git
??


copy certs:


??
**server/install.sh
sudo systemctl stop transcription.service
sudo systemctl stop chat.service
??

vi ./mrrubato/server/transcription/server.py
python3 ./mrrubato/server/transcription/server.py

https://host:8001/transcribe/?videoid=UUUoYYW7SsE

https://host:8001/transcribe/?videoid=UUUoYYW7SsE


copy transcription output
copy ./data/transcription/output 

Load all transcripts:
python3 ./mrrubato/server/ollama/loadall.py


python3 ./mrrubato/server/ollama/server.py
https://host:8000/api/?query=whats%20around%20the%20river%20bend
https://host:8000/api/?query=whats%20around%20the%20river%20bend


journalctl -u transcription.service
journalctl -u transcription.service -f



Customizable data ingestion.  
News -> scrapy ->
-> Summarization tool.  


Customize own summary.  
with Preference Feedback loop 

**news.py
Yep:
https://github.com/codelucas/newspaper
Try this library
pip3 install newspaper3k
<!--
>>> import newspaper

>>> cnn_paper = newspaper.build('http://cnn.com')

>>> for article in cnn_paper.articles:
>>>     print(article.url)
http://www.cnn.com/2013/11/27/justice/tucson-arizona-captive-girls/
http://www.cnn.com/2013/12/11/us/texas-teen-dwi-wreck/index.html
...

>>> for category in cnn_paper.category_urls():
>>>     print(category)

http://lifestyle.cnn.com
http://cnn.com/world
http://tech.cnn.com
...

>>> cnn_article = cnn_paper.articles[0]
>>> cnn_article.download()
>>> cnn_article.parse()
>>> cnn_article.nlp()

...
>>> article.keywords
['New Years', 'resolution', ...]

>>> article.summary
'The study shows that 93% of people ...'

>>> article.text

>>> article.authors
['Leigh Ann Caldwell', 'John Honway']

>>> article.publish_date
datetime.datetime(2013, 12, 30, 0, 0)

-->

Then generate a page with summary.  
Trigger through timestep.py


**SOC
Is this the start of some new literacy for me?  
Depends how far I get.  

**ROLI
Bluetooth connection is it working?

**web/public/rec.html
**web/public/record/
New entry.  
Get from testdoodle.html
but more work on this first.  


git clone --mirror https://github.com/DevinDJdj/mrrubato.git
mrrubato.git\objects\pack


**web/public/testvoice.html
**web/public/game/testllm.html
**web/public/game/speech.js
voices = this.getVoices();
utterance.voice = voices[1];
This is a list of voices from the OS and Chrome I guess.  


Need to pass context.  

#https://github.com/mlc-ai/web-llm?tab=readme-ov-file
Probably much better library, but time. ...
--start here.
https://github.com/mlc-ai/web-llm/tree/main/examples/simple-chat-js

**web/public/test/web-llm/index.html
Oh this is great.  
In-browser will become one of the primary use-cases for these models.  
This will be a key piece of software.  

Change the engine to this one.  

Try a sample use case with this tool, searching/reading the news.  
Integrate speechsynth and keymap control.  
Try without backend if possible.  

**web/public/testdoodle.html
Finish this first.  

**web/public/languages.js
Dont like this initLangData, loadLanguage etc.  
Hmmm... somewhat better.  
loadDictionaries() can only be called once per user.  

Lets use , between keys.  

**web/public/speech.js
Need to keep context somewhere, but how?  
Is this best place?  


**web/public/testdoodle.html
As we doodle, we should keep provenance chain in some way.  
mode line (..addl modifier)
mode box

mode polar
mode grid

Make general function for parsing tokens and checking token values at certain index.  
basically a cmd line interface.  
#https://github.com/GregRos/parjs

try browserify...
Too many useful node.js libraries that dont have "Quickstart" for webpage integration.  
Need a tool to integrate this output while packaging the node.js library.  
Why is this browserify not integrated into the build of the node.js library?  
npm publish.  This should browserify if this is present.  And publish along with the 
https://www.npmjs.com/package/xxxxx
Create a package for this.  For now just to test.  But potentially to allow for usage.  
Ugh....
So much to do.  


Shouldnt rule this out for convenience.  
browserify main.js -o bundle.js




#https://neo4j.com/labs/genai-ecosystem/neoconverse/
#https://dgraph.io/docs/


#https://github.com/kerighan/kinbaku

#https://github.com/graphistry/pygraphistry
What do we want to utilize this for?  
Visualizing relationship between Time and word nodes.  
Need to create a few samples and visualize to see if makes sense.  


**/web/public/test/word2vec-demo/index.html
Started here with the visualization tool at least.  


**analyze/graph.py
What plagarism detection algorithms are available.  
Similar to the desired mechanism.  
--start here.  
#https://hasanaboulhasan.medium.com/advanced-plagiarism-detector-using-python-and-ai-4-methods-2e8a4e0b0243


**book/definitions.txt
Use re-reference indicator ## or #REFCOUNT#..

**analyze/book.py
**web/public/book.html
Concentrate on the web part.  
Ingest via github links, and add to the LLM query. 

**web/public/test/web-llm/index.html

##https://github.com/mlc-ai/web-llm/tree/main/examples/chrome-extension

Copy from ##testdoodle.html
#https://llm.mlc.ai/docs/deploy/webllm.html#bring-your-own-model-variant
Not exactly what I want.  
But..


**server/ollama/custom.py
Lets see if we can make a custom model here instead of langchain RAG.  
**server/ollama/custom.parameter

##k39a--Tu4h0
Hmmm... we can already pass this dynamically.  
##server/ollama/server.py

We have up to 1MB context.  
Just pass the whole text?  
Yes, dont even bother with RAG for this.  Just use context.  

#https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k

Not default.  
Random sample as context?  
Interesting idea.  
For now this will do I think.  
Context windows will go up and this may be enough for now.  
Soon an entire life's work will be passed in a context window of an LLM...

**web/public/book.html
copy 
**web/public/test/web-llm/index.html
combine with 
**web/public/testdoodle.html
Speech/TTS



**web/public/book.html
we have a sample.  
Need to read files from github and pass this as context.  

**news.py
Same with newspaper-3k.  
Just have batch read all text.  
Insert data into firebase.  
Same analysis process during interaction.  
Context: Just have heirarchies of text with summary of ALL.  
Then summary of all plus details of related.  
etc.  


Hmm..
pending command is removed when using keys.  
Not sure if this is optimal.  

**speech.js
checkCommands a bit of a mess.  


Lots of changes.  
Hopefully didnt break anything.  

**web/public/book.html
Read from github, 
Formulate query...
use web-LLM functionality.  

Should really put the initial firebase functions into a JS file as well.  
There is more room for shared code.  

Also there must be some speechtotext model we can run the same way.  
javascript speech to text
well, just use built-in one, maybe it is good enough for our purposes.  
It will get better.  
If not just load a model in the browser:
##https://www.tensorflow.org/js/guide/save_load
##web/public/test/word2vec-demo/scripts/main.js



Commands to adjust time.  
Need to create video control commands.  


**languages/videocontrol.js 
Already have some of this in "meta"
but should really organize it better.  
Should we / can we prioritize longer instructions?  
We can use pedal, and then prioritize longest instruction first.  

Take what we can out of "meta" language.  

Eventually "add word" or "edit word" should allow for adjusting the "language" js script function via in browser codeeditor.  
Update in browser instance and then save if accepted.  



**web/public/book.html
Read from github, 
Formulate query...
use web-LLM functionality.  
--basic functionality there.  

Why when we lose keyboard focus we lose SpeechSynthesis?  
Can we get better synthesis?  
Just use browser for all this stuff.  
Eventually this is where STT will be.  
Why is everyone writing all of this backend STT services etc?  

Result is kind of neat.  
Adjust prompt to be a bit nicer.  
Need formatting function.  
Format the text so it is more readable.  
Replace URLs with HREF links, and pick up just the text used in the HREF in speechSynth.  
May want to adjust prompt to create a summary at beginning.  
Maybe only read the summary.  

