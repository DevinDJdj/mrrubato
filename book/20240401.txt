https://github.com/rhasspy/piper
https://github.com/rhasspy/piper/blob/master/TRAINING.md
run into issues
ERROR: Could not find a version that satisfies the requirement piper-phonemize~=1.1.0 (from piper-tts) (from versions: none)

https://github.com/rhasspy/piper-phonemize
Do we want to go through the trouble of building this.  
OK, look for other solutions?  
What other packages exist.  


OK, lets try this one.
try coqui TTS
wsl --distribution VENV
https://www.linuxcapable.com/install-python-3-8-on-ubuntu-linux/
pip install pip setuptools wheel tts
tts-server --list_models
tts-server --model_name tts_models/en/jenny/jenny  
http://localhost:5002/

That worked to start.  


python3 -m venv .
source ./bin/activate
pip3 install pip setuptools wheel --upgrade
pip3 install -r requirements.txt
https://github.com/coqui-ai/TTS

git clone https://github.com/coqui-ai/TTS
/test/tts/coqui/TTS

python3 ./setup.py install

Tacotron2 based model training?  

Check input DATA:
--HERE--
test/tts/coqui/TTS/TTS/bin/compute_statistics.py --config_path (config.json) --out_path (stats_path) 

run 
TTS/recipes/ljspeech/download_ljspeech.sh

Edit metadata.csv to necessary files.  
Should be > 100 files for test/train split.  


then:
TTS/recipes/ljspeech/tacotron2-DDC/train_tacotron_ddc.py
This is quite slow.  
No response for 1 minute, waiting...

torch.cuda.OutOfMemoryError: CUDA out of memory.
config = Tacotron2Config(  # This is the config that is saved for the future use
    audio=audio_config,
    batch_size=8,
    eval_batch_size=2,
    num_loader_workers=1,
    num_eval_loader_workers=1,

shorten your wav files
Or use short samples.  

ah good, these two things worked and I am running on 6GB GPU.  
Slowly.  30 mins / epoch.  
Suppose we can just leave it training, how do we resume?  
Or will it automatically? 
Tacatron2Config
output_path=output_path,

"restore_path":"run-April-03-2024_12+01PM-dbf1a08a/best_model.pth",
How do we pass this?  

**train_tacotron_ddc.py
# Load checkpoint if checkpoint_path given:
CHECKPOINT = "./run-April-03-2024_12+01PM-dbf1a08a/best_model.pth"
#CHECKPOINT = "/root/.local/share/tts/tts_models--en--ljspeech--tacotron2-DDC/model_file.pth" #config.json also here, do we need?  
if CHECKPOINT is not None or CHECKPOINT != "":
     with open(CHECKPOINT, "rb") as fh:
        buf = io.BytesIO(fh.read())
     check = torch.load(buf, map_location=torch.device("cuda"))
     model.load_state_dict(check["model"])


This way we train using past iterations as a start.  
Run for a while and see if we get anything intelligible.  
If not, use prior model and just merge with this when we have data.  



--model_name 

python3 ./synthesize.py --text "I am determined to acquire language." --model_path "../../recipes/ljspeech/tacotron2-DDC/run-April-03-2024_12+01PM-dbf1a08a/best_model.pth" --config_path "../../recipes/ljspeech/tacotron2-DDC/run-April-03-2024_12+01PM-dbf1a08a/config.json"
This works, but output is junk with 6th checkpoint file.  
Lets see if we can get a better result before moving forward.  
Why is it 11 minutes long?  


"I am determined to acquire language.  "





I guess this will give us the model/checkpoint.  
Then we would just need to overwrite the LJSpeech info.  
and rerun.  
Go through this first.  


config.json (where is this?)

stats_path -> "output.npy"
test_sentences_file -> "sentences.txt"
Just anything in this.  
datasets -> LJSpeech directory.  



**server/tts/train.py --initeration "false" --videos "xxx,yyy,zzz"
In here download all needed (to start just a few files)
along with the transcript files.  
Split the wav file at the transcription point.  
Utilize all the transcript files.  
If the transcript time is outside iteration, take the sample.  
Before analyzing if this is good enough, make sure we have human corrected data.  
We can keep the Times and splits coming from transcription service.  
We can use the samples between iterations I think ok as well.  




initeration or not.  Need to detect voice probably or do we?  



**server/tts/server.py
Call the TTS/bin/synthesize.py
and return the wav or best format file.  
call locally from **server/ollama/server.py or just call after we get response in the browser.  
Response speed will improve gradually.  

We can update these transcript files from **web/public/analyze.html
make separate transcription entry to indicate fixed data.  

**server/transcription/server.py
->api->update

**web/public/server/tts.html
->tts/server.py->api->train.py

becoming spaghetti?  


TRAIN here:
test/tts/coqui/TTS/TTS/tts

test/tts/coqui/TTS/TTS/bin/train_tacotron.py


Synthesize here:
test/tts/coqui/TTS/TTS/bin/synthesize.py


I dont think we need to make this iterative.  
It is fine to do this once every year or so.  


