https://github.com/rhasspy/piper
https://github.com/rhasspy/piper/blob/master/TRAINING.md
run into issues
ERROR: Could not find a version that satisfies the requirement piper-phonemize~=1.1.0 (from piper-tts) (from versions: none)

https://github.com/rhasspy/piper-phonemize
Do we want to go through the trouble of building this.  
OK, look for other solutions?  
What other packages exist.  


OK, lets try this one.
try coqui TTS
wsl --distribution VENV
https://www.linuxcapable.com/install-python-3-8-on-ubuntu-linux/
pip install pip setuptools wheel tts
tts-server --list_models
tts-server --model_name tts_models/en/jenny/jenny  
http://localhost:5002/

That worked to start.  


python3 -m venv .
source ./bin/activate
pip3 install pip setuptools wheel --upgrade
pip3 install -r requirements.txt
https://github.com/coqui-ai/TTS

git clone https://github.com/coqui-ai/TTS
/test/tts/coqui/TTS

python3 ./setup.py install

Tacotron2 based model training?  

Check input DATA:
--HERE--
test/tts/coqui/TTS/TTS/bin/compute_statistics.py --config_path (config.json) --out_path (stats_path) 

run 
TTS/recipes/ljspeech/download_ljspeech.sh

Edit metadata.csv to necessary files.  
Should be > 100 files for test/train split.  


then:
TTS/recipes/ljspeech/tacotron2-DDC/train_tacotron_ddc.py
This is quite slow.  
No response for 1 minute, waiting...

torch.cuda.OutOfMemoryError: CUDA out of memory.
config = Tacotron2Config(  # This is the config that is saved for the future use
    audio=audio_config,
    batch_size=8,
    eval_batch_size=2,
    num_loader_workers=1,
    num_eval_loader_workers=1,

shorten your wav files
Or use short samples.  

can we use this?  
from model directory?  
recipes/ljspeech/tacotron2-DDC/run-April-03-2024_04+21PM-dbf1a08a
../../../../TTS/bin/train_tts.py --continue_path .
See if this works.  

ah good, these two things worked and I am running on 6GB GPU.  
Slowly.  30 mins / epoch.  
Suppose we can just leave it training, how do we resume?  
Or will it automatically? 
Tacatron2Config
output_path=output_path,

"restore_path":"run-April-03-2024_12+01PM-dbf1a08a/best_model.pth",
How do we pass this?  

**train_tacotron_ddc.py
--Dont actually need this IF you are just manually doing.  
USE ../../../../TTS/bin/train_tts.py --continue_path .
# Load checkpoint if checkpoint_path given:
CHECKPOINT = "./run-April-03-2024_12+01PM-dbf1a08a/best_model.pth"
#CHECKPOINT = "/root/.local/share/tts/tts_models--en--ljspeech--tacotron2-DDC/model_file.pth" #config.json also here, do we need?  
if CHECKPOINT is not None or CHECKPOINT != "":
     with open(CHECKPOINT, "rb") as fh:
        buf = io.BytesIO(fh.read())
     check = torch.load(buf, map_location=torch.device("cuda"))
     model.load_state_dict(check["model"])


This way we train using past iterations as a start.  
Run for a while and see if we get anything intelligible.  
If not, use prior model and just merge with this when we have data.  



--model_name 
wsl --distribution VENV
test/tts/coqui/TTS/TTS/bin
python3 ./synthesize.py --model_name "tts_models/en/ljspeech/tacotron2-DDC" --text "I am determined to acquire language."
python3 ./synthesize.py --text "I am determined to acquire language." --model_path "../../recipes/ljspeech/tacotron2-DDC/run-April-03-2024_04+21PM-dbf1a08a/best_model.pth" --config_path "../../recipes/ljspeech/tacotron2-DDC/run-April-03-2024_04+21PM-dbf1a08a/config.json"
This works, but output is junk with 6th checkpoint file.  
Apparently need 10,000 steps to even see if it is working.  
hmmm... that's like 3 weeks or more on my old machine.  
Well....
in the meantime try to see if we can generate the files we need for this.  
This is the important part.  



we can run the same in server.py
server.py --config_path ... --model_path ...


Lets see if we can get a better result before moving forward.  
Why is it 11 minutes long?  
test/tts/coqui/TTS/recipes/ljspeech/tacotron2-DDC/run-April-03-2024_04+21PM-dbf1a08a
tensorboard --logdir=.

"I am determined to acquire language.  "





I guess this will give us the model/checkpoint.  
Then we would just need to overwrite the LJSpeech info.  
and rerun.  
Go through this first.  


config.json (where is this?)

stats_path -> "output.npy"
test_sentences_file -> "sentences.txt"
Just anything in this.  
datasets -> LJSpeech directory.  



**server/tts/train.py --initeration "false" --videos "xxx,yyy,zzz"
In here download all needed (to start just a few files)
along with the transcript files.  
Split the wav file at the transcription point.  
Utilize all the transcript files.  
If the transcript time is outside iteration, take the sample.  
Before analyzing if this is good enough, make sure we have human corrected data.  
We can keep the Times and splits coming from transcription service.  
We can use the samples between iterations I think ok as well.  




initeration or not.  Need to detect voice probably or do we?  



**server/tts/server.py
Call the TTS/bin/synthesize.py
and return the wav or best format file.  
call locally from **server/ollama/server.py or just call after we get response in the browser.  
Response speed will improve gradually.  

We can update these transcript files from **web/public/analyze.html
make separate transcription entry to indicate fixed data.  

**server/transcription/server.py
->api->update

**web/public/server/tts.html
->tts/server.py->api->train.py

becoming spaghetti?  


TRAIN here:
test/tts/coqui/TTS/TTS/tts

test/tts/coqui/TTS/TTS/bin/train_tacotron.py


Synthesize here:
test/tts/coqui/TTS/TTS/bin/synthesize.py


I dont think we need to make this iterative.  
It is fine to do this once every year or so.  



How big does this tacotron2-DDC/.../events file get?  
currently 6GB after ~50 epochs.  

Falling into a bit of a rabbit hole.  
50/day at the lower rate as well.  
Want to do with cheap spare parts basically, but ...

Not sure this is going to be fast enough.  
The output is all garbage.  


**server/transcription/transcribe.py
pass parameters here with iteration information.  
Extract wav files here after downloading mp3 info.  
This is still intertwined with youtube.  


And add entry to the training metadata.  
WAVNAME|TEXT|LOWERCASETEXT

LJ004-0008|Mr. Neild, a second Howard,|Mr. Neild, a second Howard,
coqui/TTS/recipes/ljspeech/tacotron2-DDC/../LJSpeech-1.1/
wavs/
metadata.csv


