Eventually we should be playing from the data we have.  
record.html or whatever and use this info to actually show the music 
and potentially based on the feedback, update the page?  
Probably too slow.  
But either way, the record.py should bring up record.html which will have 
a variety of statistics on the song last iterations and the other things
which are done via analyze.py
We do need some ability to play back and build on top of existing layers though.  
Select layers and then utilize that to play with as part of the record.py process?  
This is significantly ahead.  
First of all need to work on analyze.py to create some more interesting information.  

maybe save the analytics as data.  
This is easier than looking at the images.  

Find the latest output for a certain song, and then run the analyze before we do the record?  

OK, so uploading the images to analyze/xxx.jpg
Now display this in analyze.html if we have this.  

Wow much harder than I thought that would be adding to analyze.html.  
I had to use firebase storage functions because the URL by itself doesnt work.  
Anyway, I now have something to look at, but probably want to adjust the piano roll to be vertical and show it in time with the video.  
Or just leave as is and make a vertical version as well so we can play with that.  
It is not actually that interesting of a problem.  As long as you have an image, you can just display it and scroll through it.  
Unless we will analyze midi in the html page, dont even bother with those libraries.  
The image is sufficient, and just scroll with video time.  


Lets try to find the most prevalent rhythm patterns and the note patterns.  

Lets try to improve the speech recognition quality.  
Maybe we can get lyrics too.  

Libraries to try:
Pyaudio
librosa
SpeechRecognition

