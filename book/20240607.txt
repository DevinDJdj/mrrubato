
**server/transcription/transcribe.py
set up and use this if possible.  Or create separate transcribe2.py
https://github.com/coqui-ai/STT/blob/main/notebooks/train_personal_model_with_common_voice.ipynb
https://medium.com/visionwizard/train-your-own-speech-recognition-model-in-5-simple-steps-512d5ac348a5


**web/public/analyze.html
id="transcript" 
Save same result we get to 
/words/YYYY/file
And /misterrubato/[videoid]/transcript/
When updated via UI, update this file contents as well.  
Perhaps need .xxx ms as well


**web/public/recent.html
Need recent watch as well as created.  
Add watch parameter
watch=true

sort by: 
/watch/[videoid]/snippet/addedAt

add value:
/watch/[videoid]/comments/[uid]/pctWatched

Use these two parameters to sort results.  


**web/public/analyze.html
**web/public/keymap.js
Need midi command to mute the sound output from feedback in case this is already audible.  
Mute/unmute
mute = 12,6,6,12,0
mute output = 12,6,6,12,1
set volume 100% = 12,6,6,12,24
additional pause/play with set speed.  
pause = 0,1,1,0,0
play = 0,1,1,0,1
add commands to meta track.  
Need volume control for feedback.  
Need a display of the midi feedback in real-time, have a x second window to generate.  
Create possibility to associate color with words.  
Assign color with keys.  
Similar function
"change color for " word (color, 12-12)



**analyze/analyze.py
launch process to do transcription via server.  
Exchange this for current local client process.  


**web/public/analyze.html
test filter functionality


**web/public/keymap.js
How do we make this more ordered?  

**web/public/chat.html
**web/public/analyze.html
Show user icon of logged in user.  
Check how prompting works and change user function.  
Annoying but I think we need it.  


**server/transcription/loadall.py
re-transcribe all data after changing back to whisper-large.  
Or changing to STT mechanism.  

**server/transcription/transcribe.py
Need to be able to load from MEDIAFILE.  
/transcribe/?videoid=sdtc1to0CM8&mediafile=https://storage.googleapis.com/misterrubato-test.appspot.com/videos/2024-06-04%2022-00-17.mp4&st=141,541,949,1321&et=529,885,1286,1657
--fixed I think.  
**timestep.py
test.  


**record.py
change to use this transcription
/transcribe/?videoid=sdtc1to0CM8&mediafile=https://storage.googleapis.com/misterrubato-test.appspot.com/videos/2024-06-04%2022-00-17.mp4&st=141,541,949,1321&et=529,885,1286,1657
Save to text file.  
pull from timestep.py (535)
change transcribe_me to point to separate function same as in timestep.py
--ok maybe it works.  
Need to watch timestep.py and analyze.py if any problems.  
analyze.py still not using new **util.py

**web/public/analyze.html


**server/transcription/server.py
also add transcriptfile=xxx
when this is passed, use this as the local file to pass to ollama.  
When not present dont pass, use the mediafile, when this not present use youtube videoid.  
Do check that the URL passed is allowed.  
--done, needs test

**timestep.py 
upload new transcriptfile if updated not null.  
--done

This is all still messy


**web/public/analyze.html
Need to start showing events.  
need to add 'transcript' and 'meta' tracks.  
Flag for including transcript/meta events in flow.  
Then when skipping, search for next word (with filter) and/or meta/transcript event.  
Then show the event details.  


**server/ollama/load.py
adjust to use REVIEWED topic when we have reviewed the transcript.  

**web/public/chat.html
use topic REVIEWED in future to see what difference this makes.  
yeah, then we get all transcript and REVIEWED separately.  

Not adding topics for other transcription yet.  
How do we do this?  In analyze.html add topics/labels.  

**record.py use Groupname as topic.  

In order for this to have much meaning, we need to figure out how to use a minimal base LLM.  
With enough data, probably doesnt matter, but without a large amount of data, we mostly get the LLM data which is not all that interesting for our use-case.  


**web/public/languages.js
statefeedback.  
Need to dynamically generate this from the transcript.  
In transcript we can see the change language etc.  
So each language will have a function to do this that will be dynamically loaded if it exists.  
If doesnt exist, use the base statefeedback function.  
This will be like a dashboard.  
Should we make separate transcripts based on language?  
I think no, but we may want to change color of the text based on the language color.  

**web/public/tags.js
logic for tagging.  
Need autocomplete here.  
Pull from /topics in DB.  
Then have pointer to any videos underneath this tag name.  
This will be added to topics parameter when ingesting to ollama.  
topics must start with certain
Should we add uid who create word/tag etc?  

Finish loadTags from DB and remove tag speech.  
addTag and removeTag need to touch UI.  


