
**server/transcription/transcribe.py
set up and use this if possible.  Or create separate transcribe2.py
https://github.com/coqui-ai/STT/blob/main/notebooks/train_personal_model_with_common_voice.ipynb
https://medium.com/visionwizard/train-your-own-speech-recognition-model-in-5-simple-steps-512d5ac348a5


**web/public/analyze.html
id="transcript" 
Save same result we get to 
/words/YYYY/file
And /misterrubato/[videoid]/transcript/
When updated via UI, update this file contents as well.  
Perhaps need .xxx ms as well


**web/public/recent.html
Need recent watch as well as created.  
Add watch parameter
watch=true

sort by: 
/watch/[videoid]/snippet/addedAt

add value:
/watch/[videoid]/comments/[uid]/pctWatched

Use these two parameters to sort results.  


**web/public/analyze.html
**web/public/keymap.js
Need midi command to mute the sound output from feedback in case this is already audible.  
Mute/unmute
mute = 12,6,6,12,0
mute output = 12,6,6,12,1
set volume 100% = 12,6,6,12,24
additional pause/play with set speed.  
pause = 0,1,1,0,0
play = 0,1,1,0,1
add commands to meta track.  
Need volume control for feedback.  
Need a display of the midi feedback in real-time, have a x second window to generate.  
Create possibility to associate color with words.  
Assign color with keys.  
Similar function
"change color for " word (color, 12-12)



**analyze/analyze.py
launch process to do transcription via server.  
Exchange this for current local client process.  


**web/public/analyze.html
test filter functionality


**web/public/keymap.js
How do we make this more ordered?  

**web/public/chat.html
**web/public/analyze.html
Show user icon of logged in user.  
Check how prompting works and change user function.  
Annoying but I think we need it.  


**server/transcription/loadall.py
re-transcribe all data after changing back to whisper-large.  
Or changing to STT mechanism.  

**server/transcription/transcribe.py
Need to be able to load from MEDIAFILE.  
/transcribe/?videoid=sdtc1to0CM8&mediafile=https://storage.googleapis.com/misterrubato-test.appspot.com/videos/2024-06-04%2022-00-17.mp4&st=141,541,949,1321&et=529,885,1286,1657
--fixed I think.  
**timestep.py
test.  


**record.py
change to use this transcription
/transcribe/?videoid=sdtc1to0CM8&mediafile=https://storage.googleapis.com/misterrubato-test.appspot.com/videos/2024-06-04%2022-00-17.mp4&st=141,541,949,1321&et=529,885,1286,1657
Save to text file.  
pull from timestep.py (535)
change transcribe_me to point to separate function same as in timestep.py
--ok maybe it works.  
Need to watch timestep.py and analyze.py if any problems.  
analyze.py still not using new **util.py

**web/public/analyze.html


**server/transcription/server.py
also add transcriptfile=xxx
when this is passed, use this as the local file to pass to ollama.  
When not present dont pass, use the mediafile, when this not present use youtube videoid.  
Do check that the URL passed is allowed.  
--done, needs test

**timestep.py 
upload new transcriptfile if updated not null.  
--done

This is all still messy


**web/public/analyze.html
Need to start showing events.  
need to add 'transcript' and 'meta' tracks.  
Flag for including transcript/meta events in flow.  
Then when skipping, search for next word (with filter) and/or meta/transcript event.  
Then show the event details.  


**server/ollama/load.py
adjust to use REVIEWED topic when we have reviewed the transcript.  

**web/public/chat.html
use topic REVIEWED in future to see what difference this makes.  
yeah, then we get all transcript and REVIEWED separately.  

Not adding topics for other transcription yet.  
How do we do this?  In analyze.html add topics/labels.  

**record.py use Groupname as topic.  

In order for this to have much meaning, we need to figure out how to use a minimal base LLM.  
With enough data, probably doesnt matter, but without a large amount of data, we mostly get the LLM data which is not all that interesting for our use-case.  


**web/public/languages.js
statefeedback.  
Need to dynamically generate this from the transcript.  
In transcript we can see the change language etc.  
So each language will have a function to do this that will be dynamically loaded if it exists.  
If doesnt exist, use the base statefeedback function.  
This will be like a dashboard.  
Should we make separate transcripts based on language?  
I think no, but we may want to change color of the text based on the language color.  

**web/public/tags.js
logic for tagging.  
Need autocomplete here.  
Pull from /topics in DB.  
Then have pointer to any videos underneath this tag name.  
This will be added to topics parameter when ingesting to ollama.  
topics must start with certain
Should we add uid who create word/tag etc?  

Finish loadTags from DB and remove tag speech.  
addTag and removeTag need to touch UI.  
--done, maybe working ok.  

Carefule with DB mechanism.  Can delete data if not careful.  
Need to use separate ref variable name as this is used in the promise afterward.  


Why are we getting this in transcript:
set speed 1.50 (01:55)
set speed  (01:56)

Test timestep to make sure nothing broke.  


Can we use word2vec somehow with the tagging or vocabulary?  
Find similar tag suggestions based on the input?  
Or if we find something similar enough just use that as the tag?  
Have some default similarity parameter so we dont end up with too many tags.  


JS example:
https://www.misterrubato.com/test/word2vec-demo/index.html
combine/train with custom data
python example:
https://www.analyticsvidhya.com/blog/2023/07/step-by-step-guide-to-word2vec-with-gensim/


**web/public/midi.js
forgot what are we doing with the original midi track?  
getFile()
OK we are loading but not using.  We can use.  Mostly it is valid unless there is some overlay which JZZ doesnt parse correctly.  
So if we want to use this, we can just adjust this:
			if (feedback == true && (smf[trknum][i][0] == 144 || smf[trknum][i][0] == 128)){ //midicommands ON OFF
How do we want to use the original MIDI?  


**server
Try Deepspeech insead of Coqui for STT/TTS training.  
There is some poor compatibility with the NVIDIA driver that Coqui causes.  
Functionality is the same.  


**/web/public/keymap.js
Need an ending Pre or postfix.  
Probably need to stick with pre-fix only strategy.  
This will probably be easier programming-wise

**timestep.py
Would be convenient to have ollamaload function
with date to load anything after date=20240518.  
For when we inevitably screw up the data.  

**server/transcription/server.py
Need update of STT to use Deepspeech or similar.  
I guess try this:
https://medium.com/visionwizard/train-your-own-speech-recognition-model-in-5-simple-steps-512d5ac348a5

whisper is not good enough.  
Get workflow working first.  
Generate wav -> CSV after fixing.  


**server/transcription/transcribe.py
maybe return and then make different function for getting wav files.  


Test skipVideo
--looks ok.  

**web/public/analyze.html
Show the User Icon/name.  
Show name on hover.  ah who cares.  
How do we distinguish the original author?  
We should hold this in the video structure.  
Just add to record.  
Test:
MscGMds1wn4

OK, multiple users ok now, but how do we distinguish between them in the feedbackImage?  



**record.py 
add UID of creator in DB structure.  


I mean they will have comments, maybe list by comment date.  


Test

Show meta commands and filter for that based on input.  


UI Original Transcript is actually the edited transcript.  
Original Computer generated transcript is saved, but not displayed anywhere.  


**server/transcription/server.py
why is this getting killed sometimes?  
Got rid of the wav file generation, except for when explicitly called.  
I think this is what we want anyway.  

