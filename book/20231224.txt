**web/public/chat.html
Need to finish this.  
Just have a simple scrolling chat with smart agent.  
Calling the ollama/server.py
And Some videos which are sources to the right.  
Snippets of the videos, and link to analyze.html
*Some feedback to rate the accuracy of the answer.  *
But not entirely sure how to use the feedback yet.  
--OK we have a start.  
Still need to cache 10,000 or so results, show this in the chat page.  
RTDB is fine I think, but we probably dont want this re-queried every time it changes.  
--So not sure if we can do this easily.  
have a datatable/filter for recent results.  
have some sort of voting mechanism for query/result.  
sort by most popular etc or most recent.  
Custom chat interface, make links for all timestamps.  
Auto-load one of the sources.  

Right now I dont think we have any context from previous questions.  
This is something we should add.  But not right now.  
Try to use this:
https://github.com/webrtc/samples
We also have screen share possibility.  
More complex than I want really, but need a component, worth it?
https://codelabs.developers.google.com/codelabs/webrtc-web#1




**/server/ollama/load.py
Need to finish the process.  
This needs to be ongoing.  Something needs to kick off the load.py
for the newly ingested data.  
--OK, looks to be working.  



**web/public/analyze.html
Show the selected language and some common words.  
Show the switch language as well.  
All of this should just be simple piano fingering images.  
Also when something is played, have a scrolling translation?  
Or just show that it is a known word.  
Have a few listed and then a popup to show/search many more.  
We could have a language as well for each embedding?  

Language goes to the right of the graph.  
Show what we are playing at least on the midi.  
Can we get this along with pass to the other?  
https://jazz-soft.net/demo/Echo.html
Load very small keyboards which is what is being played.  
Show this from the midi file.  
Have separate? one for midi input
Then show a few common words.  
Keep the word/idea dictionary in the realtimedb.  


Get rid of the top duplicate voice input.  
Pull the transcript from the DB if it is there.  


*How do we associate sequences with meaning?  
*How do we make new sequence->meaning definitions?  


**web/public/language.html
This page first should show the languages as they exist and allow for editing by any admin.  
Must keep a history of the language, so every word should have an update date, and we can go back through the history of backups
to see how this evolved?  
This needs to have the small piano image of the word itself so that we have a good understanding.  
Also the sound, a variety of sounds as right now the word is only a combination of notes in sequence.  
But no rhythm attributed.  
Should also be able to create words within the language.  
Perhaps this should be a universal word.  
Then afterward attribute a "definition" to that word.  
Probably easier that the interface to be the piano interface when thinking of how to form words.  


**web/public/metalanguage.html
This should be the language switching structure.  
This also could be different, maybe nobody would change if designed well I suspect.  


either way languages should be stored in the DB and not settings files.  

First need to scan the pieces played for potential words.  
Have a workflow to give meaning to those words while listening.  
Highlight the words while listening.  
Only take the most prevalent from any particular song.  
**analyze.py
This should be done during analyze.py
Already have something close to this.  
Put this in the Language DB, then have a location for words -> Song -> iterations
Then we can go between songs as well.  
How significant is length of word?  We need to do some analysis before defining word lengths.  
I dont know that a single word length is what we want.  Perhaps we do need multiple word lengths and then a 
metalanguage switch between word lengths.  
Perhaps a few standard word lengths.  What can we use as word delimiter if we do this?  

Word -> Song/video -> iteration
Language -> Song/video -> iteration

Feedback should be part of the language

https://data.commoncrawl.org/crawl-data/CC-MAIN-2023-50/index.html


Need to get back to the language
**web/public/languages.html


**server/ollama/server.py
How do we prioritize more recent entities within the model?  


webrtc backend:
chat.misterrubato.com
pip install websocket-server
This is good enough:
https://github.com/Pithikos/python-websocket-server

Then use simple example like this:
https://github.com/heyletscode/webrtc-videocall-javascript
if we want to do this.  

So anyone who is on at the same time with the same video, we want to get audio streams.  
**web/public/analyze.html

Have a live section, and then all other comments/users are displayed in time.  
This doesnt help with the chat.html UI.  


**web/public/chat.html
Seems to work a bit better now.  
For some reason allow embedding is off for some videos.  
Cant find the pattern at the moment.  
So many annoying issues.  


**DB/
Should make a wrapper for DB calls.  In case we need to change something.  
Also Youtube maybe make a wrapper for this as well.  
Practical or not?  


