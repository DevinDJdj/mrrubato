**web/public/chat.html
Need to finish this.  
Just have a simple scrolling chat with smart agent.  
Calling the ollama/server.py
And Some videos which are sources to the right.  
Snippets of the videos, and link to analyze.html
Some feedback to rate the accuracy of the answer.  
But not entirely sure how to use the feedback yet.  
--OK we have a start.  
Still need to cache 10,000 or so results, show this in the chat page.  
RTDB is fine I think, but we probably dont want this re-queried every time it changes.  
--So not sure if we can do this easily.  
have a datatable/filter for recent results.  
have some sort of voting mechanism for query/result.  
sort by most popular etc or most recent.  
Custom chat interface, make links for all timestamps.  
Auto-load one of the sources.  


**/server/ollama/load.py
Need to finish the process.  
This needs to be ongoing.  Something needs to kick off the load.py
for the newly ingested data.  


**web/public/analyze.html
Show the selected language and some common words.  
Show the switch language as well.  
All of this should just be simple piano fingering images.  
Also when something is played, have a scrolling translation?  
Or just show that it is a known word.  
Have a few listed and then a popup to show/search many more.  
We could have a language as well for each embedding?  

Language goes to the right of the graph.  
Show what we are playing at least on the midi.  
Can we get this along with pass to the other?  
https://jazz-soft.net/demo/Echo.html
Load very small keyboards which is what is being played.  
Show this from the midi file.  
Have separate? one for midi input
Then show a few common words.  
Keep the word/idea dictionary in the realtimedb.  


Get rid of the top duplicate voice input.  
Pull the transcript from the DB if it is there.  


*How do we associate sequences with meaning?  
*How do we make new sequence->meaning definitions?  

