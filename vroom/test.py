#pip install google-api-python-client
#npm install -g firebase-tools

#category 10 is music
#ffmpeg -i "2022-12-13 12-32-17.mkv" -codec copy "2022-12-13 12-32-17.mp4"
#python ./test.py --file="C:/Users/dj/videos/2023-02-28 12-49-18.mkv" --function compress
#2:30 -> 0:40
#takes 3 mins.  
#python ./test.py --file="C:/Users/dj/videos/EKS_Deploy_nitroabc_2023-04-20 08-33-25.mkv" --function compress
#42:00 -> 7:20
#takes 60 mins (started 10:50am)
#python ./test.py --file="C:/projects/vroom/IBM_AD_Demo_2017_05_03.mp4" --function compress
#python ./test.py --file="C:/Users/dj/videos/2023-02-28 12-49-18.mkv" --function dups
#python ./test.py --file="C:/Users/dj/videos/2023-07-21 09-05-48_ws_summary.mkv" --function compress
#python ./test.py --file="C:/Users/dj/videos/2023-07-21 09-05-48_ws_summary.mkv" --function frames

#should really be doing QRCodes to be able to ingest info from screen captures.  
#this could be a general way to improve support.  
#https://pypi.org/project/qrcode/
#generate QRCode in any particular screenshot or have as an icon to include info regarding the context and any important OS or env info.  
#print out some times

#!/usr/bin/python
#getting stuff for image processing.  
#pip install opencv-python
#pip install youtube_transcript_api #this is just for the transcript stuff that is there generated by google.  
#pip install ffmpeg
#choco install ffmpeg
#copy C:\ProgramData\chocolatey\lib\ffmpeg\tools\ffmpeg\bin -> C:\Windows\System32
#pip install moviepy
#pip install pydub
#pip install speechrecognition

#import httplib
import httplib2
import os
import random
import sys
import time
import cv2
import pytesseract
from PIL import Image
import numpy as np
import string

import moviepy.editor as mp
from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips
import moviepy.video.fx.all as vfx

import wave
from pydub import AudioSegment
from pydub.silence import detect_silence, split_on_silence  #this is used for silence, but not as easy to use as it should be.  
import soundfile as sf
import pyrubberband as pyrb
import pyaudio


import speech_recognition as sr
import math

from apiclient.discovery import build
from apiclient.errors import HttpError
from apiclient.http import MediaFileUpload
from oauth2client.client import flow_from_clientsecrets
from oauth2client.file import Storage
from oauth2client.tools import argparser, run_flow

from scipy.io.wavfile import read
import numpy as np
import matplotlib.pyplot as plt
from numpy.lib.stride_tricks import sliding_window_view

import datetime

import torch

from transformers import AutoTokenizer, AutoModelWithLMHead

# Explicitly tell the underlying HTTP transport library not to retry, since
# we are handling retry logic ourselves.
httplib2.RETRIES = 1

# Maximum number of times to retry before giving up.
MAX_RETRIES = 10

# Always retry when these exceptions are raised.
RETRIABLE_EXCEPTIONS = (httplib2.HttpLib2Error, IOError)
#, httplib.NotConnected,
#  httplib.IncompleteRead, httplib.ImproperConnectionState,
#  httplib.CannotSendRequest, httplib.CannotSendHeader,
#  httplib.ResponseNotReady, httplib.BadStatusLine)

# Always retry when an apiclient.errors.HttpError with one of these status
# codes is raised.
RETRIABLE_STATUS_CODES = [500, 502, 503, 504]

# The CLIENT_SECRETS_FILE variable specifies the name of a file that contains
# the OAuth 2.0 information for this application, including its client_id and
# client_secret. You can acquire an OAuth 2.0 client ID and client secret from
# the Google API Console at
# https://console.cloud.google.com/.
# Please ensure that you have enabled the YouTube Data API for your project.
# For more information about using OAuth2 to access the YouTube Data API, see:
#   https://developers.google.com/youtube/v3/guides/authentication
# For more information about the client_secrets.json file format, see:
#   https://developers.google.com/api-client-library/python/guide/aaa_client_secrets
CLIENT_SECRETS_FILE = "./../client_secrets.json"

# This OAuth 2.0 access scope allows an application to upload files to the
# authenticated user's YouTube channel, but doesn't allow other types of access.
YOUTUBE_UPLOAD_SCOPE = "https://www.googleapis.com/auth/youtube.upload"
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"

# This variable defines a message to display if the CLIENT_SECRETS_FILE is
# missing.
MISSING_CLIENT_SECRETS_MESSAGE = """
WARNING: Please configure OAuth 2.0

To make this sample run you will need to populate the client_secrets.json file
found at:

   %s

with information from the API Console
https://console.cloud.google.com/

For more information about the client_secrets.json file format, please visit:
https://developers.google.com/api-client-library/python/guide/aaa_client_secrets
""" % os.path.abspath(os.path.join(os.path.dirname(__file__),
                                   CLIENT_SECRETS_FILE))

VALID_PRIVACY_STATUSES = ("public", "private", "unlisted")

VALID_FUNCTIONS = ("compress", "summarize", "dups", "frames")


#for now just see what frames I am playing and then convert that to time.  
#any time that my hand is on the piano, we count as playing.  
#anything longer than 5 seconds with no hands on piano is not playing.  

#get the names of songs from here.  
#https://nanonets.com/blog/ocr-with-tesseract/
#install tesseract.  
#pip install pytesseract
#https://github.com/UB-Mannheim/tesseract/wiki


def rotate_image(image, angle):
  image_center = tuple(np.array(image.shape[1::-1]) / 2)
  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)
  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)
  return result
  
def get_timestamp(s):
    mins = math.floor(s/60)
    secs = math.floor(s - mins*60)
    filler = ""
    if secs < 10:
        filler = "0"
    return str(mins) + ":" + filler + str(secs)
    
    
def moving_average(x, w, step=0.001):
    conv = np.arange(0, w, step)
    return np.convolve(x, conv, 'valid') / (w/step)
    
def consecutive(data, stepsize=1):
    return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)
    

def summarize_me(filename): #pass text file from STT
#https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70
#https://www.turing.com/kb/5-powerful-text-summarization-techniques-in-python
#try T5 from google.  
#dont want to use stuff which needs API key at the moment.  
#https://www.kdnuggets.com/2023/04/text-summarization-development-python-tutorial-gpt35.html
#this looks like a horrible summary.  

    tokenizer = AutoTokenizer.from_pretrained('t5-base')
    model = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)
    f = open(filename, "r")
    text = f.read()
    fn = filename.split('.')
    summary_fname = fn[0] + "_sum.txt"

    inputs = tokenizer.encode("summarize: " + text, return_tensors='pt',max_length=512,truncation=True)


    summary_ids = model.generate(inputs, max_length=150, min_length=80, length_penalty=5., num_beams=2)


    summary = tokenizer.decode(summary_ids[0])
    f = open(summary_fname, "w")
    f.write(summary)
    f.close()

    return summary_fname
    

#get word density
def get_word_density(filename, stretch=1, return_pct=0.1):
#create some sort of text density to try to find key points in the video.  
#not sure we can get something from the screens itself.  
#this could be part of the way we generate the summary as well.  Wherever text density is highest.  
    AudioSegment.ffmpeg = "C:\ProgramData\chocolatey\lib\ffmpeg\tools\ffmpeg\bin\ffmpeg"
    
    fn = filename.split('.')

    my_clip = mp.VideoFileClip(filename)

    temp_file = fn[0] + ".mp3"
    if (my_clip.audio is None):
        return ""
    my_clip.audio.write_audiofile(temp_file)

    #subprocess.call(['ffmpeg', '-i', temp_file + ".mp3",
    #                   temp_file + ".wav"])

    audio_filename = fn[0] + ".wav"
    txt_filename = fn[0] + "_wd.txt"
    print(temp_file)
    sound = AudioSegment.from_mp3(temp_file)
    sound.export(audio_filename, format="wav")
    audio = AudioSegment.from_file(audio_filename, format="wav", frame_rate=44100)
    audio = audio.set_channels(1)
    audio = audio.set_frame_rate(16000)
    audio.export(audio_filename, format="wav")    
    
    #this is much more accurate but has no timestamps.  
    r = sr.Recognizer()
    test = sr.AudioFile(audio_filename)
    audio_length = audio.duration_seconds
    print(audio_length)
    segment_length = 10
    number_of_iterations = int(audio_length/segment_length)
    audior = []
    text = []
    times = []
    timesorig = []
    wordlengths = []
    for i in range(number_of_iterations):
        with test as source:
            audior.append(r.record(source, offset=i*segment_length, duration=segment_length))
    for i in range(number_of_iterations):
        print(audior[i])
        try:
            text.append(r.recognize_google(audior[i], show_all=False)) 
            times.append(get_timestamp(i*segment_length/stretch))
            timesorig.append(i*segment_length/stretch)
            wordlengths.append(len(text[-1])) #have to use last in list in case we dont have anything for some periods.  
            #print(text) 
        except Exception as e:
            print(e)
            
    sortedwordlengths = np.argsort(wordlengths)
    
    wordstart = []
    wordend = []
    f = open(txt_filename, "w")
    for k in sortedwordlengths:
        if (k > len(sortedwordlengths)*(1-return_pct)):
            wordstart.append(timesorig[k])
            wordend.append(timesorig[k] + segment_length/stretch)
        print(wordlengths[k])
        print(times[k])
        print(text[k])
        f.write(' (' + times[k] + ') ' + text[k] + '\n')
    f.close()
        
    wordstart = np.sort(wordstart)
    wordend = np.sort(wordend)
    return wordstart, wordend

    
def transcribe_me(filename, stretch=1):

    AudioSegment.ffmpeg = "C:\ProgramData\chocolatey\lib\ffmpeg\tools\ffmpeg\bin\ffmpeg"
    
    fn = filename.split('.')

    my_clip = mp.VideoFileClip(filename)

    temp_file = fn[0] + ".mp3"
    if (my_clip.audio is None):
        return ""
    my_clip.audio.write_audiofile(temp_file)

    #subprocess.call(['ffmpeg', '-i', temp_file + ".mp3",
    #                   temp_file + ".wav"])

    audio_filename = fn[0] + ".wav"
    txt_filename = fn[0] + ".txt"
    print(temp_file)
    sound = AudioSegment.from_mp3(temp_file)
    sound.export(audio_filename, format="wav")
    audio = AudioSegment.from_file(audio_filename, format="wav", frame_rate=44100)
    audio = audio.set_channels(1)
    audio = audio.set_frame_rate(16000)
    audio.export(audio_filename, format="wav")    
    
    #this is much more accurate but has no timestamps.  
    r = sr.Recognizer()
    test = sr.AudioFile(audio_filename)
    audio_length = audio.duration_seconds
    print(audio_length)
    segment_length = 20
    number_of_iterations = int(audio_length/segment_length)
    audior = []
    text = []
    times = []
    for i in range(number_of_iterations):
        with test as source:
            audior.append(r.record(source, offset=i*segment_length, duration=segment_length))
    for i in range(number_of_iterations):
        print(audior[i])
        try:
            text.append(r.recognize_google(audior[i], show_all=False)) 
            times.append(get_timestamp(i*segment_length/stretch))
            #print(text) 
        except Exception as e:
            print(e)
            
    f = open(txt_filename, "w")
    for i in range(len(text)):
        print(times[i])
        print(text[i]) 
#        f.write(times[i] + '\n')
        f.write(' (' + times[i] + ') ' + text[i] + '\n')
    f.close()

    return txt_filename

def audio_remove_dups(audio_filename, dupstart, dupend, silencestart, silenceend, fps = 60):
    audio = AudioSegment.from_file(audio_filename, format="wav")
    
#    dupend.pop()  #remove last end (rearrange for keeping clips.  
    dupend.insert(0, 0) #
    dupstart.append(len(audio))
    current = audio[:10]
    for start, end in zip(dupend, dupstart):
        segment = audio[round(start*1000):round(end*1000)]
        current = current + segment
    # writing the file
    
    current.export(audio_filename, format="wav")

    return audio_filename
    #for now just remove the same dupstart and dupend.  
    
def audio_generate_summary(audio_filename, sumstart, sumend, fps = 60):
    audio = AudioSegment.from_file(audio_filename, format="wav")
    
#    dupend.pop()  #remove last end (rearrange for keeping clips.  
    current = audio[:10]
    for start, end in zip(sumstart, sumend):
        segment = audio[round(start*1000):round(end*1000)]
        current = current + segment
    # writing the file
    
    current.export(audio_filename, format="wav")

    return audio_filename
    #for now just remove the same dupstart and dupend.  
    
    
def video_generate_summary(filename, sumstart, sumend, fps=60):
    cap = cv2.VideoCapture(filename)
    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')
    writer = cv2.VideoWriter('temp_summary.mp4', fourcc, fps, (w, h))
    idx_sum = 0
    count = 0.0
    print(sumstart)
    print(sumend)
    while cap.isOpened() and count < video_length:
        # Extract the frame
        ret, frame = cap.read()
        count = count + 1
        if not ret:
            continue
        prev_frame = frame
        if (idx_sum < len(sumstart) and count/fps > sumstart[idx_sum] and count/fps < sumend[idx_sum]):
            writer.write(frame)
        elif (idx_sum < len(sumstart) and count/fps > sumend[idx_sum]):
            idx_sum = idx_sum + 1
    cap.release()


    writer.release() 
    return 'temp_summary.mp4'

#test to start.  
#here want to combine audio as well.  
#nothing working yet.  
def video_remove_dups(filename, dupstart, dupend, silencestart, silenceend, fps=60):
    """
    Writes frames to an mp4 video file
    :param file_path: Path to output video, must end with .mp4
    :param frames: List of PIL.Image objects
    :param fps: Desired frame rate
    """
    cap = cv2.VideoCapture(filename)
    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')
    writer = cv2.VideoWriter('temp_shortened.mp4', fourcc, fps, (w, h))
    idx_dup = 0
    count = 0.0
    while cap.isOpened() and count < video_length:
        # Extract the frame
        ret, frame = cap.read()
        count = count + 1
        if not ret:
            continue
        prev_frame = frame
        if (idx_dup >= len(dupstart) or count/fps < dupstart[idx_dup]):
            writer.write(frame)
        elif (idx_dup < len(dupstart) and count/fps > dupend[idx_dup]):
            writer.write(frame)
            idx_dup = idx_dup + 1
    cap.release()


    writer.release() 
    return 'temp_shortened.mp4'
"""    
    my_clip = mp.VideoFileClip(filename)
    keep_clips = []
    dupend.pop()  #remove last end (rearrange for keeping clips.  
    dupend.insert(0, 0) #
    
    for start, end in zip(dupend, dupstart):
         clip = my_clip.subclip(start, end)
         clip.ipython_display()
         keep_clips.append(clip)
         
    
#    edited_video = concatenate_videoclips(keep_clips)
    return None #edited_video
"""    


def times_to_frames(dupstart, dupend, fps = 30):
    idx = 0
    frames = []
    for s, e in zip(dupstart, dupend):
        while (idx*(1.0/fps) < s):
            frames.append(0)
            idx = idx + 1
        while (idx*(1/fps) < e):
            frames.append(1)
            idx = idx + 1
    
    return frames

def frames_to_times(frames, fps = 30):
    dupstart = []
    dupend = []
    dups = np.argwhere(frames == 1)
    print(dups.flatten())
    dupsgrouped = consecutive(dups.flatten())
#    print(np.shape(silentgrouped))
    dupcount = 0
    dupsstart = []
    dupsend = []

    for d in dupsgrouped:
        if (len(d) > 5): #5 frames
            dupcount += len(d)
            start = d[0] / fps
            end = (d[0] + len(d)) / fps
            dupsstart.append(start)
            dupsend.append(end)
            
    return dupsstart, dupsend
    
def adjust_dups(dupstart, dupend, silencestart, silenceend):
#each frame is 1/30 lets say, so lets go by the silence for now.  
#both of these are adjusted to 5 frames as minimum.  
    returndupstart = []
    returndupend = []
    dup_idx = 0
    
    #lets just progress by 1 frame at a time.  
    #if both silent and dup, we drop the frame.  
    #start simple.  
    #build frames from dups.  We should really adjust this and do this in the earlier functions
    #try to match the non-silent frames with the 
    dupframes = times_to_frames(dupstart, dupend)
    silenceframes = times_to_frames(silencestart, silenceend)
    
    print("SILENCEFRAMES: {}".format(len(np.argwhere(silenceframes == 1))))
    print("VIDFRAMES: {}".format(len(np.argwhere(dupframes == 1))))
    #what if different length?  
    currentstate = 0
    count = 0
    while (dup_idx < len(silenceframes) and dup_idx < len(dupframes)):
        if (silenceframes[dup_idx] == dupframes[dup_idx] and silenceframes[dup_idx] == 1):
            count = count + 1
        if (silenceframes[dup_idx] == 1 and dupframes[dup_idx] == 1 and currentstate == 0):
            currentstate = 1
            returndupstart.append(dup_idx*(1/30))
        if ((silenceframes[dup_idx] == 0 or dupframes[dup_idx] == 0) and currentstate == 1):
            currentstate = 0
            returndupend.append(dup_idx*(1/30))
        dup_idx = dup_idx + 1
            
    
    #ugh only a few dup frames.  
    print("DUPFRAMES: {}".format(count))
    if (len(returndupstart) > len(returndupend)):
        if len(silenceframes) > len(dupframes):
            returndupend.append(len(dupframes)*(1/30))
        else:
            returndupend.append(len(silenceframes)*(1/30))
    return returndupstart, returndupend        
        



#https://medium.com/prog-ramming-solutions/python-music-playback-speed-1x-2x-3x-change-without-chipmunk-effect-890eb10826c1
def compress_me(filename, out_loc, stretch=1.5):
#mp.VideoFileClip 
#It can have any extension supported by ffmpeg: .ogv, .mp4, .mpeg, .avi, .mov, .m4v
    print(datetime.datetime.now())
    fn = filename.split('.')
    my_clip = mp.VideoFileClip(filename)
    transcript_file = transcribe_me(filename)
    print(datetime.datetime.now())
    temp_file = fn[0] + ".mp3"
    final_file = fn[0] + "_s.mp4"
    summary_file = fn[0] + "_s_sum.mp4"
    noaudio = False
    if (my_clip.audio is not None):
        my_clip.audio.write_audiofile(temp_file)
        audio_filename = fn[0] + ".wav"
        txt_filename = fn[0] + ".txt"
        print(temp_file)
        sound = AudioSegment.from_mp3(temp_file)
        sound.export(audio_filename, format="wav")

    #use these two and match them and then cut anything overlapping.  
    #cant take all the silence out.  Also cant take all the dup frames out.  
    #what is a good starting point?  
    #this is like an auto-post-processing.  
    #need to speed up the video check though.  Maybe just check every 2 frames.  
    #should I just try to use a tool for this?  
    #descript for instance.  
    #youtube vanced
    #sponsorblock
    
        silencesstart, silencesend = get_silences(audio_filename)
        print(silencesstart)
        print(silencesend)
    else:
        noaudio = True
        silencesstart = []
        silencesend = []        

    print(datetime.datetime.now())

    dupsstart, dupsend = get_dup_indexes(filename, out_loc) #40 mins
    print(dupsstart)
    print(dupsend)

    print(datetime.datetime.now())
#ok we have a start.  
#so now we need to adjust the dupstart/dupend, separate function to essentially combine 
#dupstart, dupend, silencestart, silenceend
    dupsstart, dupsend = adjust_dups(dupsstart, dupsend, silencesstart, silencesend)
    #print(zip(dupsstart, dupsend))
    #seems this is taking long time, maybe just do each several frames.  No need to check every frame?  
    edited_video = video_remove_dups(filename, dupsstart, dupsend, silencesstart, silencesend)
    my_clip = mp.VideoFileClip(edited_video)
    
    print(datetime.datetime.now())
    if noaudio == False:    
        edited_audio = audio_remove_dups(audio_filename, dupsstart, dupsend, silencesstart, silencesend)
        y, sr = sf.read(edited_audio)
    #    y, sr = librosa.load(audio_filename)
    #    y_stretch = librosa.effects.time_stretch(y, stretch)    
        # Play back at 1.5X speed
        y_stretch = pyrb.time_stretch(y, sr, stretch)
        # Play back two 1.5x tones
        y_shift = pyrb.pitch_shift(y_stretch, sr, stretch)
        sf.write("temp" + str(stretch) + ".wav", y_stretch, sr, format='wav')    
        sound = AudioSegment.from_wav("temp" + str(stretch) + ".wav")
        sound.export(out_loc + "temp" + str(stretch) + ".mp3", format="mp3")
    else:
        print("No Audio Detected")

    print(datetime.datetime.now())

    clip = VideoFileClip(edited_video)
    edited_audio = AudioFileClip(edited_audio)
    clip = clip.set_audio(edited_audio)
    clip.write_videofile(final_file) #40 mins
    #transcription doesnt work with sped up file.  Hopefully it works with clipped file.  
    transcription = transcribe_me(final_file, stretch)
    summary_start, summary_end = get_word_density(final_file, stretch)
    print(datetime.datetime.now())
    
    print("fps: {}".format(clip.fps))

    # Modify the FPS
    clip = clip.set_fps(clip.fps * stretch)
    

    # Apply speed up
    final = clip.fx(vfx.speedx, stretch)

    print(datetime.datetime.now())

    print("fps: {}".format(final.fps))
    

    audio_final = out_loc + "temp" + str(stretch) + ".mp3"
    if noaudio == False:
        #add audio
        audio = AudioFileClip(audio_final)
        final = final.set_audio(audio)
    
    # Save video clip
    #final.write_videofile(out_loc + "temp" + str(stretch) + ".mp4")
    final.write_videofile(final_file) #40 mins

    print(datetime.datetime.now())
    #transcription = transcribe_me(final_file)
    
    summary_video = video_generate_summary(final_file, summary_start, summary_end)
    summary_clip = VideoFileClip(summary_video)
    if noaudio == False:    
        summary_audio = audio_generate_summary("temp" + str(stretch) + ".wav", summary_start, summary_end)
        sound = AudioSegment.from_wav("temp" + str(stretch) + ".wav")
        sound.export(out_loc + "temp" + str(stretch) + ".mp3", format="mp3")
        summary_audio_final = out_loc + "temp" + str(stretch) + ".mp3"
        edited_summary_audio = AudioFileClip(summary_audio_final)
        summary_clip = summary_clip.set_audio(edited_summary_audio)
    summary_clip.write_videofile(summary_file)        
#    summary = summarize_me(transcript_file)

def mse(img1, img2):
   h, w, rgb = img1.shape
   diff = cv2.subtract(img1, img2)
   err = np.sum(diff**2)
   mse = err/(float(h*w*rgb))
   return mse
   
def get_dup_indexes(input_loc, output_loc):
#this is a bit too slow.  About same time as the video.  
#perhaps just do every other frame.  
#so combine this with the audio and see what we can compress.  
#wonder what this sounds like.  
    cap = cv2.VideoCapture(input_loc)
    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1
    clip = mp.VideoFileClip(input_loc)
    slength = video_length / clip.fps
    frame_indexes = np.empty(video_length+1)
    print ("Number of frames: ", video_length)
    count = 0
    prev_frame = None
    while cap.isOpened() and count < video_length:
        # Extract the frame
        ret, frame = cap.read()
        count = count + 1
        if not ret:
            continue
        # Write the results back to output location.
        if (prev_frame is not None):
#            print(prev_frame.shape)
            diff = mse(prev_frame, frame)
            if (diff < 2):
                frame_indexes[count] = 1
            else:
                frame_indexes[count] = 0
        prev_frame = frame
    cap.release()
    # Print stats
    print ("Done extracting frames.\n%d frames extracted" % count)
    print ("Duplicates: %d" % ((frame_indexes == 1).sum()))
    print ("Unique: %d" % ((frame_indexes == 0).sum()))
    

    dups = np.argwhere(frame_indexes == 1)
    print(dups.flatten())
    dupsgrouped = consecutive(dups.flatten())
#    print(np.shape(silentgrouped))
    dupcount = 0
    dupsstart = []
    dupsend = []

    for d in dupsgrouped:
        if (len(d) > 5): #5 frames
            dupcount += len(d)
            start = d[0] / clip.fps
            end = (d[0] + len(d)) / clip.fps
            dupsstart.append(start)
            dupsend.append(end)
            
    return dupsstart, dupsend

def get_silences(filename):
    # Read the Audiofile
    samplerate, data = read(filename)
    print(f"number of channels = {data.shape[1]}")
    # Frame rate for the Audio
    print(samplerate)

    # Duration of the audio in Seconds
    duration = len(data)/samplerate
    print("Duration of Audio in Seconds", duration)
    print("Duration of Audio in Minutes", duration/60)

    time = np.arange(0,duration,1/samplerate)
    #step here is the 1/44100, but on the graph this is all bunched.  
    # Plotting the Graph using Matplotlib
    if (data.shape[1] > 1):
        plt.plot(time, data[:, 0], label="Left channel")
        plt.plot(time, data[:, 1], label="Right channel")
    else:
        plt.plot(time,data)

    #look into the data here and see if we can get rid of excess.  
    #just flag silence by second for now.  
    #first if all frames in the second are below a certain amplitude, 
    #flag that second as a potential to discard.  
    #same for video lets do by second to start.  
    #for now just average the amplitude and remove the lower amplitude below
    #the avg - 3-stddev perhaps.  This will likely be inaudible anyway.  
#    absdata = moving_average(np.absolute(data[:, 0]), 1, 0.001)
    absdata = np.absolute(data[:,0])
    avg = np.average(absdata)
    print("Abs AVG", avg)
    stddev = np.std(absdata)
    print("STDDEV", stddev)
    #here get where there is no audio above the avg for some period of time.  
    v = sliding_window_view(absdata, 1000)
    maxval = v.max(axis=-1)
    silent = np.argwhere(maxval < avg/2)
    print(silent.flatten())
    silentgrouped = consecutive(silent.flatten())
#    print(np.shape(silentgrouped))
    silentcount = 0
    silencesstart = []
    silencesend = []
    for s in silentgrouped:
        if (len(s) > 7000): #~44100/6 5 frames
            silentcount += len(s)
            start = s[0] / samplerate
            end = (s[0] + len(s)) / samplerate
            silencesstart.append(start)
            silencesend.append(end)
    #not sure this is what I want but something like tihs.  
    #so just use this and then only use where this is consecutive for a period of time.  
    #yeah this is getting closer.  
    
    #maybe we just want random samples so we get a better spaced out sample?  
    #or maybe just where maxval is < 
    
    #silent = np.argwhere(absdata < avg)
    print("Silent seconds", silentcount/samplerate)
    
#    plt.xlabel('Time [s]')
#    plt.ylabel('Amplitude')
#    plt.title(filename)
#    plt.show()
    return silencesstart, silencesend


def video_to_frames(input_loc, output_loc):
    """Function to extract frames from input video file
    and save them as separate frames in an output directory.
    Args:
        input_loc: Input video file.
        output_loc: Output directory to save the frames.
    Returns:
        None
    """
    txt_filename = "temp_ocr.txt"
    linecontent = dict()
    fullcontent = dict()
#    times = dict()
    pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'
    try:
        os.mkdir(output_loc)
    except OSError:
        pass
    # Log the time
    time_start = time.time()
    # Start capturing the feed
    cap = cv2.VideoCapture(input_loc)
    # Find the number of frames
    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1
    print ("Number of frames: ", video_length)
    count = 0
    print ("Converting video..\n")
    # Start converting the video
    
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract'
    # Example tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract'
    while cap.isOpened():
        # Extract the frame
        ret, frame = cap.read()
        if not ret:
            continue
        # Write the results back to output location.
        count = count + 1
        if count%30==0:
            
            cv2.imwrite(output_loc + "/tempocr.jpg", frame)
            print(pytesseract.image_to_string(frame))
            temp = pytesseract.image_to_string(frame)
            lines = temp.split("\n")
            for line in lines:
                if line not in linecontent:
                    linecontent[line] = count/30
            fullcontent[count/30] = temp
            #utilize the fullcontent of anything with linecontent.  
            #the duplicates should get overwritten, and we will get the last frames with certain text.  
            #maybe we want the first frames, not sure.  
            
            # If there are no more frames left
            if (count > (video_length-1)):
                # Log the time again
                time_end = time.time()
                # Release the feed
                cap.release()
                # Print stats
                print ("Done extracting frames.\n%d frames extracted" % count)
                print ("It took %d seconds forconversion." % (time_end-time_start))
                break
    
    #ok test this see if it works.  
    f = open(txt_filename, "w")
    for c in count/30:
        temp = fullcontent[c]
        lines = temp.split("\n")
        lc = 0
        for line in lines:
            if linecontent[line] == c:
                #write context only if it is not going to be printed anyway.  
                if (lc > 0 and linecontent[ lines[lc-1] ] != c):
                    f.write(' (' + linecontent[ lines[lc-1] ] + ') ' + lines[lc-1] + '\n')
                f.write(' (' + c + ') ' + line + '\n')
                if (lc < len(lines)-1 and linecontent[ lines[lc+1] ] != c):
                    f.write(' (' + linecontent[ lines[lc+1] ] + ') ' + lines[lc+1] + '\n')
                    
            lc = lc + 1
        
    f.close()
    
            
def get_authenticated_service(args):
  flow = flow_from_clientsecrets(CLIENT_SECRETS_FILE,
    scope=YOUTUBE_UPLOAD_SCOPE,
    message=MISSING_CLIENT_SECRETS_MESSAGE)

  storage = Storage("%s-oauth2.json" % sys.argv[0])
  credentials = storage.get()

  if credentials is None or credentials.invalid:
    credentials = run_flow(flow, storage, args)

  return build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,
    http=credentials.authorize(httplib2.Http()))

def initialize_upload(youtube, options):
  tags = None
  if options.keywords:
    tags = options.keywords.split(",")

  body=dict(
    snippet=dict(
      title=options.title,
      description=options.description,
      tags=tags,
      categoryId=options.category
    ),
    status=dict(
      privacyStatus=options.privacyStatus
    )
  )

  # Call the API's videos.insert method to create and upload the video.
  insert_request = youtube.videos().insert(
    part=",".join(body.keys()),
    body=body,
    # The chunksize parameter specifies the size of each chunk of data, in
    # bytes, that will be uploaded at a time. Set a higher value for
    # reliable connections as fewer chunks lead to faster uploads. Set a lower
    # value for better recovery on less reliable connections.
    #
    # Setting "chunksize" equal to -1 in the code below means that the entire
    # file will be uploaded in a single HTTP request. (If the upload fails,
    # it will still be retried where it left off.) This is usually a best
    # practice, but if you're using Python older than 2.6 or if you're
    # running on App Engine, you should set the chunksize to something like
    # 1024 * 1024 (1 megabyte).
    media_body=MediaFileUpload(options.file, chunksize=-1, resumable=True)
  )

  resumable_upload(insert_request)

# This method implements an exponential backoff strategy to resume a
# failed upload.
def resumable_upload(insert_request):
  response = None
  error = None
  retry = 0
  while response is None:
    try:
      print ("Uploading file...")
      status, response = insert_request.next_chunk()
      if response is not None:
        if 'id' in response:
          print ("Video id '%s' was successfully uploaded." % response['id'])
        else:
          exit("The upload failed with an unexpected response: %s" % response)
    except HttpError as e:
      if e.resp.status in RETRIABLE_STATUS_CODES:
        error = "A retriable HTTP error %d occurred:\n%s" % (e.resp.status,
                                                             e.content)
      else:
        raise
    except RETRIABLE_EXCEPTIONS as e:
      error = "A retriable error occurred: %s" % e

    if error is not None:
      print (error)
      retry += 1
      if retry > MAX_RETRIES:
        exit("No longer attempting to retry.")

      max_sleep = 2 ** retry
      sleep_seconds = random.random() * max_sleep
      print ("Sleeping %f seconds and then retrying..." % sleep_seconds)
      time.sleep(sleep_seconds)





    
if __name__ == '__main__':
  argparser.add_argument("--file", required=True, help="Video file to upload")
  argparser.add_argument("--title", help="Video title", default="Test Title")
  argparser.add_argument("--description", help="Video description",
    default="Test Description")
  argparser.add_argument("--category", default="22",
    help="Numeric video category. " +
      "See https://developers.google.com/youtube/v3/docs/videoCategories/list")
  argparser.add_argument("--keywords", help="Video keywords, comma separated",
    default="")
  argparser.add_argument("--privacyStatus", choices=VALID_PRIVACY_STATUSES,
    default=VALID_PRIVACY_STATUSES[0], help="Video privacy status.")
    
  argparser.add_argument("--function", choices=VALID_FUNCTIONS,
    default=VALID_FUNCTIONS[0], help="compress or summarize.")
    
  args = argparser.parse_args()

  if not os.path.exists(args.file):
    exit("Please specify a valid file using the --file= parameter.")
  
  
  if args.function == "compress":
      compress_me(args.file, "./output/test/")
  elif args.function == "dups":
      get_dup_indexes(args.file, "./output/test/")
  else:
      transcribe_me(args.file)
      video_to_frames(args.file, "./output/test/")
  
#  youtube = get_authenticated_service(args)
  try:
    a = 2
#    initialize_upload(youtube, args)
  except HttpError as e:
    print ("An HTTP error %d occurred:\n%s" % (e.resp.status, e.content))